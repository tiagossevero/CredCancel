{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Gerador Autom√°tico de Data Schemas\n",
    "## CRED-CANCEL v3.0\n",
    "\n",
    "Este notebook executa automaticamente:\n",
    "- `DESCRIBE FORMATTED` para cada tabela\n",
    "- `SELECT * FROM <tabela> LIMIT 10` para cada tabela\n",
    "- Metadados adicionais (contagem, colunas, etc.)\n",
    "\n",
    "### Tabelas processadas:\n",
    "- **16 tabelas originais** (usr_sat_*)\n",
    "- **13 tabelas intermedi√°rias** (teste.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes\n",
    "OUTPUT_DIR = \"data_schemas\"\n",
    "\n",
    "# Criar diret√≥rios\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/originais\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/intermediarias\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Diret√≥rios criados em: {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Spark Session\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "# Usar sess√£o existente ou criar nova\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    spark = SparkSession(sc)\n",
    "    print(f\"‚úÖ Usando SparkSession existente (Spark {spark.version})\")\n",
    "except:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataSchema Generator\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    print(f\"‚úÖ Nova SparkSession criada (Spark {spark.version})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Defini√ß√£o de Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defini√ß√£o de todas as tabelas\n",
    "TABELAS_ORIGINAIS = {\n",
    "    'usr_sat_ods': [\n",
    "        'ods_decl_dime_raw',\n",
    "        'vw_cad_contrib',\n",
    "        'vw_ods_pagamento',\n",
    "        'vw_sna_pgdasd_grupo_empresarial',\n",
    "        'vw_ods_contrib',\n",
    "        'vw_ods_dcip'\n",
    "    ],\n",
    "    'usr_sat_cadastro': [\n",
    "        'ruc_protocolo',\n",
    "        'ruc_general',\n",
    "        'tab_sit_cad'\n",
    "    ],\n",
    "    'usr_sat_shared': [\n",
    "        'tab_generica',\n",
    "        'tab_munic'\n",
    "    ],\n",
    "    'usr_sat_auditoria': [\n",
    "        'aud_empresa_sob_suspeita',\n",
    "        'aud_empresa_suspeita'\n",
    "    ]\n",
    "}\n",
    "\n",
    "TABELAS_INTERMEDIARIAS = {\n",
    "    'teste': [\n",
    "        'credito_dime',\n",
    "        'credito_dime_completo',\n",
    "        'credito_dime_textil',\n",
    "        'credito_dime_metalmec',\n",
    "        'credito_dime_tech',\n",
    "        'cancel_cnpj',\n",
    "        'cancel_cadastro',\n",
    "        'cancel_recolhimento',\n",
    "        'cancel_suspeitas',\n",
    "        'cancel_suspeitas_score',\n",
    "        'cancel_zero_normal',\n",
    "        'cancel_zero_simples',\n",
    "        'cancel_final'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Contar tabelas\n",
    "total_originais = sum(len(v) for v in TABELAS_ORIGINAIS.values())\n",
    "total_intermediarias = sum(len(v) for v in TABELAS_INTERMEDIARIAS.values())\n",
    "total_geral = total_originais + total_intermediarias\n",
    "\n",
    "print(f\"üìä Total de tabelas a processar: {total_geral}\")\n",
    "print(f\"   - Originais: {total_originais}\")\n",
    "print(f\"   - Intermedi√°rias: {total_intermediarias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Fun√ß√µes Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_resultado(conteudo, tipo, database, tabela):\n",
    "    \"\"\"Salva resultado em arquivo\"\"\"\n",
    "    categoria = \"originais\" if database != \"teste\" else \"intermediarias\"\n",
    "    filepath = f\"{OUTPUT_DIR}/{categoria}/{database}.{tabela}_{tipo}.txt\"\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(conteudo)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "\n",
    "def formatar_describe(df):\n",
    "    \"\"\"Formata output do DESCRIBE FORMATTED\"\"\"\n",
    "    linhas = []\n",
    "    linhas.append(\"=\" * 100)\n",
    "    linhas.append(\"DESCRIBE FORMATTED\")\n",
    "    linhas.append(\"=\" * 100)\n",
    "    linhas.append(\"\")\n",
    "    \n",
    "    for row in df.collect():\n",
    "        col_name = str(row[0]) if row[0] else \"\"\n",
    "        data_type = str(row[1]) if row[1] else \"\"\n",
    "        comment = str(row[2]) if len(row) > 2 and row[2] else \"\"\n",
    "        \n",
    "        linhas.append(f\"{col_name:<35} {data_type:<25} {comment}\")\n",
    "    \n",
    "    return \"\\n\".join(linhas)\n",
    "\n",
    "\n",
    "def formatar_select(df, tabela_nome):\n",
    "    \"\"\"Formata output do SELECT * LIMIT 10\"\"\"\n",
    "    linhas = []\n",
    "    linhas.append(\"=\" * 100)\n",
    "    linhas.append(f\"SELECT * FROM {tabela_nome} LIMIT 10\")\n",
    "    linhas.append(\"=\" * 100)\n",
    "    linhas.append(\"\")\n",
    "    \n",
    "    # Cabe√ßalho\n",
    "    colunas = df.columns\n",
    "    linhas.append(\" | \".join(colunas))\n",
    "    linhas.append(\"-\" * 100)\n",
    "    \n",
    "    # Dados\n",
    "    rows = df.collect()\n",
    "    if len(rows) == 0:\n",
    "        linhas.append(\"(Nenhum registro encontrado)\")\n",
    "    else:\n",
    "        for row in rows:\n",
    "            valores = [str(val)[:50] if val is not None else \"NULL\" for val in row]\n",
    "            linhas.append(\" | \".join(valores))\n",
    "    \n",
    "    linhas.append(\"\")\n",
    "    linhas.append(f\"Total de registros exibidos: {len(rows)}\")\n",
    "    \n",
    "    return \"\\n\".join(linhas)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes auxiliares definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Fun√ß√£o de Processamento Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_tabela(spark, database, tabela):\n",
    "    \"\"\"Processa uma tabela individual\"\"\"\n",
    "    tabela_completa = f\"{database}.{tabela}\"\n",
    "    \n",
    "    print(f\"\\nüìä Processando: {tabela_completa}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # 1. DESCRIBE FORMATTED\n",
    "        print(\"  üîç Executando DESCRIBE FORMATTED...\")\n",
    "        df_describe = spark.sql(f\"DESCRIBE FORMATTED {tabela_completa}\")\n",
    "        conteudo_describe = formatar_describe(df_describe)\n",
    "        arquivo_describe = salvar_resultado(conteudo_describe, \"describe\", database, tabela)\n",
    "        print(f\"  ‚úÖ DESCRIBE salvo: {arquivo_describe}\")\n",
    "        \n",
    "        # 2. SELECT * LIMIT 10\n",
    "        print(\"  üîç Executando SELECT * LIMIT 10...\")\n",
    "        df_select = spark.sql(f\"SELECT * FROM {tabela_completa} LIMIT 10\")\n",
    "        conteudo_select = formatar_select(df_select, tabela_completa)\n",
    "        arquivo_select = salvar_resultado(conteudo_select, \"select\", database, tabela)\n",
    "        print(f\"  ‚úÖ SELECT salvo: {arquivo_select}\")\n",
    "        \n",
    "        # 3. Metadados adicionais\n",
    "        print(\"  üìà Coletando metadados...\")\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as total FROM {tabela_completa}\").collect()[0].total\n",
    "        \n",
    "        metadata = []\n",
    "        metadata.append(\"=\" * 100)\n",
    "        metadata.append(\"METADADOS\")\n",
    "        metadata.append(\"=\" * 100)\n",
    "        metadata.append(f\"Tabela: {tabela_completa}\")\n",
    "        metadata.append(f\"Total de registros: {count:,}\")\n",
    "        metadata.append(f\"Total de colunas: {len(df_select.columns)}\")\n",
    "        metadata.append(f\"Data da extra√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        metadata.append(\"\")\n",
    "        metadata.append(\"Colunas:\")\n",
    "        for col in df_select.columns:\n",
    "            metadata.append(f\"  - {col}\")\n",
    "        \n",
    "        arquivo_metadata = salvar_resultado(\"\\n\".join(metadata), \"metadata\", database, tabela)\n",
    "        print(f\"  ‚úÖ METADATA salvo: {arquivo_metadata}\")\n",
    "        \n",
    "        print(f\"  ‚úÖ ‚úÖ ‚úÖ Conclu√≠do: {tabela_completa} ({count:,} registros)\")\n",
    "        \n",
    "        return True, count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå ERRO em {tabela_completa}: {str(e)}\")\n",
    "        \n",
    "        # Salvar log de erro\n",
    "        erro_msg = f\"ERRO ao processar {tabela_completa}\\n\"\n",
    "        erro_msg += f\"Timestamp: {datetime.now()}\\n\"\n",
    "        erro_msg += f\"Erro: {str(e)}\\n\"\n",
    "        salvar_resultado(erro_msg, \"ERRO\", database, tabela)\n",
    "        \n",
    "        return False, 0\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fun√ß√£o de processamento definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ PRIORIDADE ALTA - Tabelas do Streamlit\n",
    "\n",
    "Processando as 4 tabelas principais usadas no dashboard Streamlit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabelas de PRIORIDADE ALTA (usadas no Streamlit)\n",
    "PRIORIDADE_ALTA = [\n",
    "    ('teste', 'credito_dime_completo'),\n",
    "    ('teste', 'credito_dime_textil'),\n",
    "    ('teste', 'credito_dime_metalmec'),\n",
    "    ('teste', 'credito_dime_tech')\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ PROCESSANDO TABELAS DE PRIORIDADE ALTA\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "resultados_alta = []\n",
    "for database, tabela in PRIORIDADE_ALTA:\n",
    "    sucesso, count = processar_tabela(spark, database, tabela)\n",
    "    resultados_alta.append((database, tabela, sucesso, count))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PRIORIDADE ALTA CONCLU√çDA\")\n",
    "print(\"=\" * 80)\n",
    "for db, tab, suc, cnt in resultados_alta:\n",
    "    status = \"‚úÖ\" if suc else \"‚ùå\"\n",
    "    print(f\"{status} {db}.{tab}: {cnt:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ TABELAS ORIGINAIS (usr_sat_*)\n",
    "\n",
    "Processando todas as tabelas fonte do banco de produ√ß√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìÅ PROCESSANDO TABELAS ORIGINAIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "resultados_originais = []\n",
    "\n",
    "for database, tabelas in TABELAS_ORIGINAIS.items():\n",
    "    print(f\"\\nüóÇÔ∏è  Database: {database}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for tabela in tabelas:\n",
    "        sucesso, count = processar_tabela(spark, database, tabela)\n",
    "        resultados_originais.append((database, tabela, sucesso, count))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TABELAS ORIGINAIS CONCLU√çDAS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ TABELAS INTERMEDI√ÅRIAS (teste.*)\n",
    "\n",
    "Processando todas as tabelas intermedi√°rias criadas no processamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üîÑ PROCESSANDO TABELAS INTERMEDI√ÅRIAS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "resultados_intermediarias = []\n",
    "\n",
    "for database, tabelas in TABELAS_INTERMEDIARIAS.items():\n",
    "    print(f\"\\nüóÇÔ∏è  Database: {database}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for tabela in tabelas:\n",
    "        # Pular se j√° foi processada na prioridade alta\n",
    "        if (database, tabela) in PRIORIDADE_ALTA:\n",
    "            print(f\"‚è≠Ô∏è  Pulando {database}.{tabela} (j√° processada em PRIORIDADE ALTA)\")\n",
    "            continue\n",
    "        \n",
    "        sucesso, count = processar_tabela(spark, database, tabela)\n",
    "        resultados_intermediarias.append((database, tabela, sucesso, count))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TABELAS INTERMEDI√ÅRIAS CONCLU√çDAS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä RELAT√ìRIO FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidar resultados\n",
    "todos_resultados = resultados_alta + resultados_originais + resultados_intermediarias\n",
    "\n",
    "total_processadas = len(todos_resultados)\n",
    "total_sucesso = sum(1 for r in todos_resultados if r[2])\n",
    "total_falhas = total_processadas - total_sucesso\n",
    "total_registros = sum(r[3] for r in todos_resultados if r[2])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä RELAT√ìRIO FINAL - GERA√á√ÉO DE DATA SCHEMAS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"üìÖ Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "print(f\"üìà Total de tabelas processadas: {total_processadas}\")\n",
    "print(f\"   ‚úÖ Sucesso: {total_sucesso}\")\n",
    "print(f\"   ‚ùå Falhas: {total_falhas}\")\n",
    "print()\n",
    "print(f\"üíæ Total de registros nas tabelas: {total_registros:,}\")\n",
    "print()\n",
    "print(f\"üìÅ Arquivos salvos em: {os.path.abspath(OUTPUT_DIR)}/\")\n",
    "print()\n",
    "\n",
    "# Detalhamento por categoria\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã DETALHAMENTO POR CATEGORIA\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"üéØ PRIORIDADE ALTA (Streamlit):\")\n",
    "for db, tab, suc, cnt in resultados_alta:\n",
    "    status = \"‚úÖ\" if suc else \"‚ùå\"\n",
    "    print(f\"  {status} {db}.{tab}: {cnt:,} registros\")\n",
    "\n",
    "print()\n",
    "print(\"üìÅ TABELAS ORIGINAIS:\")\n",
    "for db, tab, suc, cnt in resultados_originais:\n",
    "    status = \"‚úÖ\" if suc else \"‚ùå\"\n",
    "    print(f\"  {status} {db}.{tab}: {cnt:,} registros\")\n",
    "\n",
    "print()\n",
    "print(\"üîÑ TABELAS INTERMEDI√ÅRIAS:\")\n",
    "for db, tab, suc, cnt in resultados_intermediarias:\n",
    "    status = \"‚úÖ\" if suc else \"‚ùå\"\n",
    "    print(f\"  {status} {db}.{tab}: {cnt:,} registros\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ PROCESSO CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Gerar Arquivo √çndice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar √≠ndice completo\n",
    "indice = []\n",
    "indice.append(\"=\" * 100)\n",
    "indice.append(\"√çNDICE DE DATA SCHEMAS - CRED-CANCEL v3.0\")\n",
    "indice.append(\"=\" * 100)\n",
    "indice.append(f\"Data de gera√ß√£o: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "indice.append(f\"Total de tabelas: {total_processadas}\")\n",
    "indice.append(f\"Processadas com sucesso: {total_sucesso}\")\n",
    "indice.append(f\"Falhas: {total_falhas}\")\n",
    "indice.append(f\"Total de registros: {total_registros:,}\")\n",
    "indice.append(\"\")\n",
    "indice.append(\"=\" * 100)\n",
    "indice.append(\"üéØ TABELAS DE PRIORIDADE ALTA (Streamlit)\")\n",
    "indice.append(\"=\" * 100)\n",
    "for db, tab, suc, cnt in resultados_alta:\n",
    "    status = \"‚úÖ\" if suc else \"‚ùå\"\n",
    "    indice.append(f\"{status} {db}.{tab} - {cnt:,} registros\")\n",
    "\n",
    "indice.append(\"\")\n",
    "indice.append(\"=\" * 100)\n",
    "indice.append(\"üìÅ TABELAS ORIGINAIS\")\n",
    "indice.append(\"=\" * 100)\n",
    "for db, tab, suc, cnt in resultados_originais:\n",
    "    status = \"‚úÖ\" if suc else \"‚ùå\"\n",
    "    indice.append(f\"{status} {db}.{tab} - {cnt:,} registros\")\n",
    "\n",
    "indice.append(\"\")\n",
    "indice.append(\"=\" * 100)\n",
    "indice.append(\"üîÑ TABELAS INTERMEDI√ÅRIAS\")\n",
    "indice.append(\"=\" * 100)\n",
    "for db, tab, suc, cnt in resultados_intermediarias:\n",
    "    status = \"‚úÖ\" if suc else \"‚ùå\"\n",
    "    indice.append(f\"{status} {db}.{tab} - {cnt:,} registros\")\n",
    "\n",
    "indice.append(\"\")\n",
    "indice.append(\"=\" * 100)\n",
    "indice.append(\"üìÅ ESTRUTURA DE ARQUIVOS\")\n",
    "indice.append(\"=\" * 100)\n",
    "indice.append(\"\")\n",
    "indice.append(\"Para cada tabela, foram gerados 3 arquivos:\")\n",
    "indice.append(\"  1. <database>.<tabela>_describe.txt  - Schema detalhado (DESCRIBE FORMATTED)\")\n",
    "indice.append(\"  2. <database>.<tabela>_select.txt    - Amostra de 10 registros (SELECT * LIMIT 10)\")\n",
    "indice.append(\"  3. <database>.<tabela>_metadata.txt  - Metadados (total registros, colunas, etc)\")\n",
    "indice.append(\"\")\n",
    "indice.append(\"Organiza√ß√£o de diret√≥rios:\")\n",
    "indice.append(f\"  {OUTPUT_DIR}/\")\n",
    "indice.append(\"    ‚îú‚îÄ‚îÄ originais/      (tabelas usr_sat_*)\")\n",
    "indice.append(\"    ‚îú‚îÄ‚îÄ intermediarias/ (tabelas teste.*)\")\n",
    "indice.append(\"    ‚îî‚îÄ‚îÄ INDEX.txt       (este arquivo)\")\n",
    "\n",
    "filepath = f\"{OUTPUT_DIR}/INDEX.txt\"\n",
    "with open(filepath, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\".join(indice))\n",
    "\n",
    "print(f\"üìã √çndice completo salvo em: {filepath}\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ Todos os data schemas foram gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclus√£o\n",
    "\n",
    "Os data schemas foram gerados com sucesso!\n",
    "\n",
    "### üìÇ Pr√≥ximos passos:\n",
    "\n",
    "1. Verifique os arquivos gerados em `data_schemas/`\n",
    "2. Revise o arquivo `INDEX.txt` para um resumo completo\n",
    "3. Use os arquivos `*_describe.txt` para documenta√ß√£o de schemas\n",
    "4. Use os arquivos `*_select.txt` para exemplos de dados\n",
    "5. Use os arquivos `*_metadata.txt` para estat√≠sticas das tabelas\n",
    "\n",
    "### üìù Formato dos arquivos:\n",
    "- `<database>.<tabela>_describe.txt` ‚Üí Schema completo (tipos, coment√°rios)\n",
    "- `<database>.<tabela>_select.txt` ‚Üí 10 primeiras linhas de dados\n",
    "- `<database>.<tabela>_metadata.txt` ‚Üí Contagem, colunas, data de extra√ß√£o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
