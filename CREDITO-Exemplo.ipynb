{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aea681-2477-45f5-9ffe-1cb8f6cb1a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf8a1f-48e4-4c57-a162-c839c8e3a940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_csv\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00777b24-69dc-4a6e-ba6f-4e11cd534937",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277a291-ac84-4dff-b123-790865d1461f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "NOTEBOOK 01: CONFIGURA√á√ÉO E CONEX√ÉO - AN√ÅLISE DE CR√âDITOS ICMS ACUMULADOS\n",
    "=============================================================================\n",
    "Auditor Fiscal: Thiago Severo\n",
    "Sistema: An√°lise de Cr√©ditos DIME com PySpark\n",
    "Base: Impala Hue + Jupyter Notebook\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ SISTEMA DE AN√ÅLISE DE CR√âDITOS ICMS ACUMULADOS - SEFAZ/SC\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üë§ Auditor: Tiago Severo - AFRE/SC\")\n",
    "print(f\"üéØ Objetivo: Detec√ß√£o de irregularidades em cr√©ditos DIME\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Criar sess√£o Spark\n",
    "spark = session.sparkSession\n",
    "\n",
    "# Verificar se as tabelas principais existem\n",
    "required_tables = [\n",
    "        'credito_dime_completo',\n",
    "        'credito_dime_textil', \n",
    "        'credito_dime_metalmec',\n",
    "        'credito_dime_tech'\n",
    "]\n",
    "\n",
    "# Verificar todas as tabelas\n",
    "print(\"\\nüîç Verifica√ß√£o detalhada das tabelas:\")\n",
    "tabelas_info = []\n",
    "for table in required_tables:\n",
    "    info = verificar_tabela('teste', table)\n",
    "    tabelas_info.append(info)\n",
    "    print(f\"  {info['tabela']}: {info['registros']:,} registros, {info['colunas']} colunas - {info['status']}\")\n",
    "\n",
    "# Criar DataFrame resumo\n",
    "df_resumo = pd.DataFrame(tabelas_info)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CONFIGURA√á√ÉO CONCLU√çDA COM SUCESSO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìå Pr√≥ximos passos:\")\n",
    "print(\"  1. Execute o Notebook 02 para An√°lise Explorat√≥ria (EDA)\")\n",
    "print(\"  2. Execute o Notebook 03 para Modelagem de Machine Learning\")\n",
    "print(\"  3. Execute o Notebook 04 para Dashboards Interativos\")\n",
    "print(\"\\nüí° Dica: Mantenha esta sess√£o ativa para os pr√≥ximos notebooks\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Salvar informa√ß√µes da sess√£o para uso nos pr√≥ximos notebooks\n",
    "session_info = {\n",
    "    'spark_version': spark.version,\n",
    "    'app_name': spark.sparkContext.appName,\n",
    "    'session_id': spark.sparkContext.applicationId,\n",
    "    'inicio': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'tabelas_verificadas': df_resumo.to_dict('records')\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Sess√£o Spark ativa e pronta para an√°lise!\")\n",
    "print(f\"   Application ID: {session_info['session_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847018b-723f-4051-aeb3-edf5d8d757bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SOLU√á√ÉO DEFINITIVA - PREVEN√á√ÉO DE CONFLITOS DE NOMES\n",
    "=============================================================================\n",
    "Execute este script NO IN√çCIO de CADA notebook para evitar conflitos\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîß CARREGANDO IMPORTS SEGUROS - PREVEN√á√ÉO DE CONFLITOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS SEGUROS COM ALIASES\n",
    "# =============================================================================\n",
    "\n",
    "# PySpark - Imports com aliases para evitar conflitos\n",
    "from pyspark.sql.functions import (\n",
    "    # Fun√ß√µes de coluna\n",
    "    col as spark_col,\n",
    "    when as spark_when,\n",
    "    lit as spark_lit,\n",
    "    \n",
    "    # Fun√ß√µes de ordena√ß√£o\n",
    "    desc as spark_desc,\n",
    "    asc as spark_asc,\n",
    "    \n",
    "    # Fun√ß√µes de agrega√ß√£o\n",
    "    count as spark_count,\n",
    "    sum as spark_sum,\n",
    "    avg as spark_avg,\n",
    "    max as spark_max,\n",
    "    min as spark_min,\n",
    "    stddev as spark_stddev,\n",
    "    \n",
    "    # Outras fun√ß√µes √∫teis\n",
    "    countDistinct,\n",
    "    log1p,\n",
    "    expr,\n",
    "    lag,\n",
    "    lead,\n",
    "    row_number,\n",
    "    rank,\n",
    "    dense_rank,\n",
    "    percent_rank,\n",
    "    ntile,\n",
    "    first,\n",
    "    last\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Python builtins - manter refer√™ncias expl√≠citas\n",
    "builtin_sum = sum\n",
    "builtin_max = max\n",
    "builtin_min = min\n",
    "\n",
    "# Pandas e NumPy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Matplotlib e Seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Outras\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n‚úÖ IMPORTS CARREGADOS COM SUCESSO!\")\n",
    "print(\"\\nüìã FUN√á√ïES DISPON√çVEIS:\")\n",
    "print(\"\\n  üîµ PySpark (use com prefixo spark_):\")\n",
    "print(\"     spark_col('nome')  - ao inv√©s de col('nome')\")\n",
    "print(\"     spark_sum('col')   - ao inv√©s de sum('col')\")\n",
    "print(\"     spark_max('col')   - ao inv√©s de max('col')\")\n",
    "print(\"     spark_when()       - ao inv√©s de when()\")\n",
    "print(\"     spark_desc()       - ao inv√©s de desc()\")\n",
    "print(\"\\n  üü¢ Python builtin (use com prefixo builtin_):\")\n",
    "print(\"     builtin_sum([1,2,3])  - ao inv√©s de sum([1,2,3])\")\n",
    "print(\"     builtin_max(a, b)     - ao inv√©s de max(a, b)\")\n",
    "print(\"\\n  üü° NumPy (para opera√ß√µes matem√°ticas):\")\n",
    "print(\"     np.maximum(a, b)   - para max entre 2 valores\")\n",
    "print(\"     np.sum(array)      - para soma de arrays\")\n",
    "\n",
    "# =============================================================================\n",
    "# FUN√á√ïES AUXILIARES PARA CONVERS√ÉO SEGURA\n",
    "# =============================================================================\n",
    "\n",
    "def converter_decimal_para_float(df_pandas, colunas=None):\n",
    "    \"\"\"\n",
    "    Converte colunas Decimal para float de forma segura\n",
    "    \n",
    "    Args:\n",
    "        df_pandas: DataFrame pandas\n",
    "        colunas: Lista de colunas (None = todas as object)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com convers√µes aplicadas\n",
    "    \"\"\"\n",
    "    if colunas is None:\n",
    "        # Detectar automaticamente colunas object\n",
    "        colunas = df_pandas.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col_name in colunas:\n",
    "        try:\n",
    "            df_pandas[col_name] = df_pandas[col_name].astype(float)\n",
    "        except (ValueError, TypeError):\n",
    "            # Coluna n√£o √© num√©rica, pular\n",
    "            pass\n",
    "    \n",
    "    return df_pandas\n",
    "\n",
    "\n",
    "def safe_aggregation(df_spark, agg_dict):\n",
    "    \"\"\"\n",
    "    Executa agrega√ß√µes de forma segura usando fun√ß√µes do PySpark\n",
    "    \n",
    "    Args:\n",
    "        df_spark: DataFrame Spark\n",
    "        agg_dict: Dict com {coluna: 'funcao'}\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame agregado\n",
    "    \"\"\"\n",
    "    agg_expressions = []\n",
    "    \n",
    "    for col_name, func in agg_dict.items():\n",
    "        if func == 'count':\n",
    "            agg_expressions.append(spark_count(col_name).alias(f\"{col_name}_{func}\"))\n",
    "        elif func == 'sum':\n",
    "            agg_expressions.append(spark_sum(col_name).alias(f\"{col_name}_{func}\"))\n",
    "        elif func == 'avg':\n",
    "            agg_expressions.append(spark_avg(col_name).alias(f\"{col_name}_{func}\"))\n",
    "        elif func == 'max':\n",
    "            agg_expressions.append(spark_max(col_name).alias(f\"{col_name}_{func}\"))\n",
    "        elif func == 'min':\n",
    "            agg_expressions.append(spark_min(col_name).alias(f\"{col_name}_{func}\"))\n",
    "    \n",
    "    return df_spark.agg(*agg_expressions)\n",
    "\n",
    "\n",
    "def safe_filter(df_spark, conditions):\n",
    "    \"\"\"\n",
    "    Aplica filtros de forma segura\n",
    "    \n",
    "    Args:\n",
    "        df_spark: DataFrame Spark\n",
    "        conditions: Dict com {coluna: (operador, valor)}\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame filtrado\n",
    "    \"\"\"\n",
    "    filtered_df = df_spark\n",
    "    \n",
    "    for col_name, (op, value) in conditions.items():\n",
    "        if op == '==':\n",
    "            filtered_df = filtered_df.filter(spark_col(col_name) == value)\n",
    "        elif op == '>':\n",
    "            filtered_df = filtered_df.filter(spark_col(col_name) > value)\n",
    "        elif op == '<':\n",
    "            filtered_df = filtered_df.filter(spark_col(col_name) < value)\n",
    "        elif op == '>=':\n",
    "            filtered_df = filtered_df.filter(spark_col(col_name) >= value)\n",
    "        elif op == '<=':\n",
    "            filtered_df = filtered_df.filter(spark_col(col_name) <= value)\n",
    "        elif op == 'in':\n",
    "            filtered_df = filtered_df.filter(spark_col(col_name).isin(value))\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFICA√á√ÉO DE CONFLITOS\n",
    "# =============================================================================\n",
    "\n",
    "def verificar_namespace():\n",
    "    \"\"\"Verifica se h√° conflitos no namespace atual\"\"\"\n",
    "    \n",
    "    import inspect\n",
    "    frame = inspect.currentframe().f_back\n",
    "    namespace = frame.f_globals\n",
    "    \n",
    "    conflitos = []\n",
    "    \n",
    "    # Verificar se fun√ß√µes cr√≠ticas existem\n",
    "    funcoes_criticas = ['col', 'sum', 'max', 'min', 'when', 'desc']\n",
    "    \n",
    "    for func in funcoes_criticas:\n",
    "        if func in namespace:\n",
    "            obj = namespace[func]\n",
    "            if isinstance(obj, str) or not callable(obj):\n",
    "                conflitos.append(f\"‚ö†Ô∏è '{func}' foi sobrescrito (tipo: {type(obj).__name__})\")\n",
    "    \n",
    "    if conflitos:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ö†Ô∏è CONFLITOS DETECTADOS NO NAMESPACE:\")\n",
    "        print(\"=\"*80)\n",
    "        for conflito in conflitos:\n",
    "            print(f\"  {conflito}\")\n",
    "        print(\"\\nüí° SOLU√á√ÉO: Restart o kernel (Kernel > Restart Kernel)\")\n",
    "        print(\"=\"*80)\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Nenhum conflito detectado no namespace\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXECU√á√ÉO AUTOM√ÅTICA\n",
    "# =============================================================================\n",
    "\n",
    "verificar_namespace()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SISTEMA PRONTO - USE AS FUN√á√ïES COM PREFIXO spark_\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "üí° EXEMPLOS DE USO:\n",
    "\n",
    "# CORRETO - Com prefixo spark_:\n",
    "df.filter(spark_col(\"idade\") > 18)\n",
    "df.groupBy(\"cidade\").agg(spark_sum(\"vendas\"))\n",
    "df.orderBy(spark_desc(\"score\"))\n",
    "\n",
    "# CORRETO - Python builtin:\n",
    "total = builtin_sum([1, 2, 3])\n",
    "maior = builtin_max(10, 20)\n",
    "\n",
    "# CORRETO - NumPy:\n",
    "max_val = np.maximum(array1, array2)\n",
    "\n",
    "# ERRADO - Causa conflito:\n",
    "for col in df.columns:  # 'col' sobrescreve a fun√ß√£o!\n",
    "    print(col)\n",
    "\n",
    "# CORRETO - Use col_name:\n",
    "for col_name in df.columns:\n",
    "    print(col_name)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüöÄ Pode executar o resto do notebook com seguran√ßa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942fdf1-777a-4726-ac75-6c6f2ec29b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "NOTEBOOK 02: AN√ÅLISE EXPLORAT√ìRIA DE DADOS (EDA) - CR√âDITOS ICMS DIME\n",
    "=============================================================================\n",
    "An√°lise completa dos cr√©ditos acumulados com visualiza√ß√µes avan√ßadas\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä AN√ÅLISE EXPLORAT√ìRIA DE DADOS - CR√âDITOS ICMS ACUMULADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CARREGAMENTO E VIS√ÉO GERAL DOS DADOS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1Ô∏è‚É£ CARREGAMENTO E VIS√ÉO GERAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Carregar tabela principal\n",
    "df_principal = spark.table(\"teste.credito_dime_completo\")\n",
    "df_principal.cache()\n",
    "\n",
    "total_empresas = df_principal.count()\n",
    "total_grupos = df_principal.select(countDistinct(\"nu_cnpj_grupo\")).collect()[0][0]\n",
    "\n",
    "print(f\"\\nüìà PANORAMA GERAL:\")\n",
    "print(f\"  ‚Ä¢ Total de empresas analisadas: {total_empresas:,}\")\n",
    "print(f\"  ‚Ä¢ Total de grupos econ√¥micos: {total_grupos:,}\")\n",
    "\n",
    "# Estat√≠sticas descritivas\n",
    "stats = df_principal.select(\n",
    "    sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    avg(\"saldo_credor_atual\").alias(\"saldo_medio\"),\n",
    "    stddev(\"saldo_credor_atual\").alias(\"saldo_desvio\"),\n",
    "    max(\"saldo_credor_atual\").alias(\"saldo_maximo\"),\n",
    "    min(\"saldo_credor_atual\").alias(\"saldo_minimo\"),\n",
    "    sum(\"vl_credito_presumido_13m\").alias(\"cp_total\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nüí∞ VALORES FINANCEIROS:\")\n",
    "print(f\"  ‚Ä¢ Saldo credor total: R$ {stats['saldo_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Saldo m√©dio por empresa: R$ {stats['saldo_medio']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Desvio padr√£o: R$ {stats['saldo_desvio']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Saldo m√°ximo: R$ {stats['saldo_maximo']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Cr√©dito presumido total: R$ {stats['cp_total']:,.2f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DISTRIBUI√á√ÉO POR CLASSIFICA√á√ÉO DE RISCO\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2Ô∏è‚É£ DISTRIBUI√á√ÉO POR CLASSIFICA√á√ÉO DE RISCO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dist_risco = df_principal.groupBy(\"classificacao_risco\").agg(\n",
    "    count(\"*\").alias(\"quantidade\"),\n",
    "    sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    avg(\"saldo_credor_atual\").alias(\"saldo_medio\"),\n",
    "    avg(\"score_risco\").alias(\"score_medio\")\n",
    ").orderBy(\n",
    "    when(col(\"classificacao_risco\") == \"CR√çTICO\", 1)\n",
    "    .when(col(\"classificacao_risco\") == \"ALTO\", 2)\n",
    "    .when(col(\"classificacao_risco\") == \"M√âDIO\", 3)\n",
    "    .otherwise(4)\n",
    ").toPandas()\n",
    "\n",
    "# Converter Decimal para float\n",
    "dist_risco['saldo_total'] = dist_risco['saldo_total'].astype(float)\n",
    "dist_risco['saldo_medio'] = dist_risco['saldo_medio'].astype(float)\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o por n√≠vel de risco:\")\n",
    "for _, row in dist_risco.iterrows():\n",
    "    perc = (row['quantidade'] / total_empresas) * 100\n",
    "    print(f\"  ‚Ä¢ {row['classificacao_risco']:<10}: {row['quantidade']:>6,} empresas ({perc:>5.1f}%)\")\n",
    "    print(f\"    Saldo total: R$ {row['saldo_total']:>15,.2f} | Score m√©dio: {row['score_medio']:>5.1f}\")\n",
    "\n",
    "# Visualiza√ß√£o: Pizza e Barras\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Distribui√ß√£o de Empresas', 'Impacto Financeiro por Risco'),\n",
    "    specs=[[{'type':'pie'}, {'type':'bar'}]]\n",
    ")\n",
    "\n",
    "# Gr√°fico de Pizza\n",
    "colors = {'CR√çTICO': '#d62728', 'ALTO': '#ff7f0e', 'M√âDIO': '#ffdd70', 'BAIXO': '#1f77b4'}\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=dist_risco['classificacao_risco'], \n",
    "           values=dist_risco['quantidade'],\n",
    "           marker_colors=[colors.get(x, '#gray') for x in dist_risco['classificacao_risco']],\n",
    "           textinfo='percent+label'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Gr√°fico de Barras\n",
    "fig.add_trace(\n",
    "    go.Bar(x=dist_risco['classificacao_risco'], \n",
    "           y=dist_risco['saldo_total']/1e6,\n",
    "           marker_color=[colors.get(x, '#gray') for x in dist_risco['classificacao_risco']],\n",
    "           text=[f'R$ {x/1e6:.1f}M' for x in dist_risco['saldo_total']],\n",
    "           textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Classifica√ß√£o de Risco\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Saldo Credor (Milh√µes R$)\", row=1, col=2)\n",
    "fig.update_layout(height=500, showlegend=False, \n",
    "                  title_text=\"An√°lise de Distribui√ß√£o por Risco\")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. AN√ÅLISE TEMPORAL E COMPORTAMENTAL\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3Ô∏è‚É£ AN√ÅLISE TEMPORAL E COMPORTAMENTAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Padr√µes de estagna√ß√£o\n",
    "comportamento = df_principal.groupBy(\n",
    "    when(col(\"qtde_ultimos_meses_iguais\") >= 12, \"‚â•12 meses congelado\")\n",
    "    .when(col(\"qtde_ultimos_meses_iguais\") >= 6, \"6-11 meses congelado\")\n",
    "    .when(col(\"qtde_ultimos_meses_iguais\") >= 3, \"3-5 meses congelado\")\n",
    "    .otherwise(\"Varia√ß√£o normal\").alias(\"padrao_comportamento\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"quantidade\"),\n",
    "    sum(\"saldo_credor_atual\").alias(\"saldo_total\")\n",
    ").orderBy(desc(\"saldo_total\")).toPandas()\n",
    "\n",
    "comportamento['saldo_total'] = comportamento['saldo_total'].astype(float)\n",
    "\n",
    "print(\"\\n‚è±Ô∏è Padr√µes de comportamento temporal:\")\n",
    "for _, row in comportamento.iterrows():\n",
    "    perc = (row['quantidade'] / total_empresas) * 100\n",
    "    print(f\"  ‚Ä¢ {row['padrao_comportamento']:<25}: {row['quantidade']:>6,} ({perc:>5.1f}%) | R$ {row['saldo_total']:>15,.2f}\")\n",
    "\n",
    "# An√°lise de crescimento\n",
    "crescimento = df_principal.groupBy(\n",
    "    when(col(\"crescimento_saldo_percentual\") > 500, \">500% (Explosivo)\")\n",
    "    .when(col(\"crescimento_saldo_percentual\") > 200, \"200-500% (Muito Alto)\")\n",
    "    .when(col(\"crescimento_saldo_percentual\") > 100, \"100-200% (Alto)\")\n",
    "    .when(col(\"crescimento_saldo_percentual\") > 50, \"50-100% (Moderado)\")\n",
    "    .when(col(\"crescimento_saldo_percentual\") > 0, \"0-50% (Baixo)\")\n",
    "    .otherwise(\"Negativo ou Zero\").alias(\"faixa_crescimento\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"quantidade\"),\n",
    "    avg(\"crescimento_saldo_percentual\").alias(\"cresc_medio\")\n",
    ").orderBy(desc(\"cresc_medio\")).toPandas()\n",
    "\n",
    "print(\"\\nüìà Distribui√ß√£o por faixa de crescimento:\")\n",
    "for _, row in crescimento.iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['faixa_crescimento']:<30}: {row['quantidade']:>6,} empresas | M√©dia: {row['cresc_medio']:>6.1f}%\")\n",
    "\n",
    "# Visualiza√ß√£o Temporal\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=comportamento['padrao_comportamento'],\n",
    "    y=comportamento['quantidade'],\n",
    "    name='Quantidade',\n",
    "    marker_color='indianred'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribui√ß√£o por Padr√£o de Comportamento Temporal\",\n",
    "    xaxis_title=\"Padr√£o\",\n",
    "    yaxis_title=\"Quantidade de Empresas\",\n",
    "    height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TOP EMPRESAS CR√çTICAS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4Ô∏è‚É£ TOP 30 EMPRESAS MAIS CR√çTICAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_criticas = df_principal.filter(\n",
    "    col(\"classificacao_risco\").isin([\"CR√çTICO\", \"ALTO\"])\n",
    ").orderBy(\n",
    "    desc(\"score_risco\"), \n",
    "    desc(\"saldo_credor_atual\")\n",
    ").limit(30).select(\n",
    "    \"nu_cnpj\",\n",
    "    \"nm_razao_social\",\n",
    "    \"nm_gerfe\",\n",
    "    \"score_risco\",\n",
    "    \"classificacao_risco\",\n",
    "    \"saldo_credor_atual\",\n",
    "    \"qtde_ultimos_meses_iguais\",\n",
    "    \"crescimento_saldo_percentual\",\n",
    "    \"vl_credito_presumido_13m\"\n",
    ").toPandas()\n",
    "\n",
    "# Converter Decimal\n",
    "for col_name in ['saldo_credor_atual', 'crescimento_saldo_percentual', 'vl_credito_presumido_13m']:\n",
    "    if col_name in top_criticas.columns:\n",
    "        top_criticas[col_name] = top_criticas[col_name].astype(float)\n",
    "\n",
    "print(\"\\nüéØ TOP 30 Empresas Priorit√°rias para Fiscaliza√ß√£o:\\n\")\n",
    "for idx, row in top_criticas.head(30).iterrows():\n",
    "    print(f\"{idx+1:2d}. {row['nm_razao_social'][:50]:<50}\")\n",
    "    print(f\"    CNPJ: {row['nu_cnpj']} | GERFE: {row['nm_gerfe']}\")\n",
    "    print(f\"    Score: {row['score_risco']:>5.0f} | Risco: {row['classificacao_risco']}\")\n",
    "    print(f\"    Saldo: R$ {row['saldo_credor_atual']:>12,.2f} | Meses iguais: {row['qtde_ultimos_meses_iguais']}\")\n",
    "    print(f\"    Crescimento: {row['crescimento_saldo_percentual']:>6.1f}% | CP 13m: R$ {row['vl_credito_presumido_13m']:>12,.2f}\")\n",
    "    print()\n",
    "\n",
    "# Visualiza√ß√£o Top 30\n",
    "fig = px.scatter(top_criticas, \n",
    "                 x='saldo_credor_atual', \n",
    "                 y='score_risco',\n",
    "                 size='qtde_ultimos_meses_iguais',\n",
    "                 color='classificacao_risco',\n",
    "                 hover_name='nm_razao_social',\n",
    "                 hover_data=['nm_gerfe', 'crescimento_saldo_percentual'],\n",
    "                 title='Top 30 Empresas Cr√≠ticas: Score vs Saldo Credor',\n",
    "                 labels={'saldo_credor_atual': 'Saldo Credor (R$)',\n",
    "                        'score_risco': 'Score de Risco',\n",
    "                        'qtde_ultimos_meses_iguais': 'Meses Estagnados'},\n",
    "                 color_discrete_map={'CR√çTICO': '#d62728', 'ALTO': '#ff7f0e'})\n",
    "\n",
    "fig.update_xaxes(type=\"log\")\n",
    "fig.update_layout(height=600)\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. AN√ÅLISE POR GER√äNCIA REGIONAL (GERFE)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5Ô∏è‚É£ AN√ÅLISE POR GER√äNCIA REGIONAL (GERFE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gerfe_analise = df_principal.groupBy(\"nm_gerfe\").agg(\n",
    "    count(\"*\").alias(\"total_empresas\"),\n",
    "    sum(when(col(\"classificacao_risco\").isin([\"CR√çTICO\", \"ALTO\"]), 1).otherwise(0)).alias(\"empresas_prioritarias\"),\n",
    "    sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    avg(\"score_risco\").alias(\"score_medio\")\n",
    ").orderBy(desc(\"empresas_prioritarias\")).toPandas()\n",
    "\n",
    "# Converter Decimal para float\n",
    "for col_name in ['saldo_total']:\n",
    "    gerfe_analise[col_name] = gerfe_analise[col_name].astype(float)\n",
    "\n",
    "print(\"\\nüó∫Ô∏è Ranking por GERFE (ordenado por empresas priorit√°rias):\\n\")\n",
    "for idx, row in gerfe_analise.head(15).iterrows():\n",
    "    perc = (row['empresas_prioritarias'] / row['total_empresas']) * 100\n",
    "    print(f\"{idx+1:2d}. {row['nm_gerfe']:<30} | Total: {row['total_empresas']:>5,} empresas\")\n",
    "    print(f\"    Priorit√°rias: {row['empresas_prioritarias']:>4,} ({perc:>5.1f}%) | Score m√©dio: {row['score_medio']:>5.1f}\")\n",
    "    print(f\"    Saldo total: R$ {row['saldo_total']:>15,.2f}\\n\")\n",
    "\n",
    "# Mapa de calor\n",
    "fig = go.Figure(data=go.Bar(\n",
    "    x=gerfe_analise.head(15)['empresas_prioritarias'],\n",
    "    y=gerfe_analise.head(15)['nm_gerfe'],\n",
    "    orientation='h',\n",
    "    marker=dict(\n",
    "        color=gerfe_analise.head(15)['score_medio'],\n",
    "        colorscale='Reds',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Score M√©dio\")\n",
    "    ),\n",
    "    text=[f\"{x:,}\" for x in gerfe_analise.head(15)['empresas_prioritarias']],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Top 15 GERFEs por Empresas Priorit√°rias (cor = score m√©dio)\",\n",
    "    xaxis_title=\"N√∫mero de Empresas Priorit√°rias\",\n",
    "    yaxis_title=\"GERFE\",\n",
    "    height=600\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. MATRIZ DE CORRELA√á√ÉO DE INDICADORES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6Ô∏è‚É£ MATRIZ DE CORRELA√á√ÉO DE INDICADORES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Selecionar features num√©ricas para correla√ß√£o\n",
    "features_correlacao = [\n",
    "    \"score_risco\",\n",
    "    \"saldo_credor_atual\", \n",
    "    \"qtde_ultimos_meses_iguais\",\n",
    "    \"crescimento_saldo_percentual\",\n",
    "    \"vl_credito_presumido_13m\",\n",
    "    \"desvio_padrao_credito\",\n",
    "    \"media_credito_13m\"\n",
    "]\n",
    "\n",
    "df_corr = df_principal.select(*features_correlacao).toPandas()\n",
    "\n",
    "# Converter colunas\n",
    "for col_name in df_corr.columns:\n",
    "    df_corr[col_name] = df_corr[col_name].astype(float)\n",
    "\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "print(\"\\nüìä Correla√ß√µes mais fortes com Score de Risco:\")\n",
    "correlacoes_score = correlation_matrix['score_risco'].sort_values(ascending=False)\n",
    "for idx, valor in correlacoes_score.items():\n",
    "    if idx != 'score_risco':\n",
    "        print(f\"  ‚Ä¢ {idx:<40}: {valor:>6.3f}\")\n",
    "\n",
    "# Heatmap de correla√ß√£o\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix.values,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale='RdBu_r',\n",
    "    zmid=0,\n",
    "    text=correlation_matrix.values,\n",
    "    texttemplate='%{text:.2f}',\n",
    "    textfont={\"size\": 10}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Matriz de Correla√ß√£o de Indicadores\",\n",
    "    height=700,\n",
    "    width=900\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AN√ÅLISE EXPLORAT√ìRIA CONCLU√çDA\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìå Principais Insights:\")\n",
    "print(f\"  1. {dist_risco[dist_risco['classificacao_risco']=='CR√çTICO']['quantidade'].values[0]:,} empresas em situa√ß√£o CR√çTICA\")\n",
    "print(f\"  2. R$ {dist_risco[dist_risco['classificacao_risco']=='CR√çTICO']['saldo_total'].values[0]:,.2f} em risco cr√≠tico\")\n",
    "print(f\"  3. {comportamento[comportamento['padrao_comportamento']=='‚â•12 meses congelado']['quantidade'].values[0]:,} empresas com valores congelados ‚â•12 meses\")\n",
    "print(f\"\\nüéØ Pr√≥ximo passo: Execute o Notebook 03 para Modelagem de Machine Learning\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9ece6-c710-41f5-90bd-12fe3026f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "NOTEBOOK 03: MACHINE LEARNING PIPELINE - PREDI√á√ÉO DE RISCOS FISCAIS\n",
    "=============================================================================\n",
    "Pipeline completo com Random Forest, XGBoost e Clustering K-Means\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ü§ñ MACHINE LEARNING PIPELINE - PREDI√á√ÉO DE RISCOS FISCAIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. PREPARA√á√ÉO DOS DADOS E FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1Ô∏è‚É£ PREPARA√á√ÉO DOS DADOS E FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_ml = spark.table(\"teste.credito_dime_completo\")\n",
    "\n",
    "# Criar features derivadas\n",
    "df_ml = df_ml.withColumn('ratio_cresc_saldo', \n",
    "                         col('crescimento_saldo_absoluto') / (col('saldo_13m_atras') + 1)) \\\n",
    "            .withColumn('intensidade_bc', log1p(col('saldo_credor_atual'))) \\\n",
    "            .withColumn('taxa_extremas', \n",
    "                       when(col('valor_igual_total_periodos') > 0,\n",
    "                            col('qtde_ultimos_meses_iguais') / col('valor_igual_total_periodos'))\n",
    "                       .otherwise(0)) \\\n",
    "            .withColumn('consistencia_periodo', \n",
    "                       col('qtde_meses_declarados_13m') / 13.0) \\\n",
    "            .withColumn('volatilidade_credito',\n",
    "                       col('desvio_padrao_credito') / (col('media_credito_13m') + 1))\n",
    "\n",
    "# Criar label bin√°ria (1 = CR√çTICO/ALTO, 0 = M√âDIO/BAIXO)\n",
    "df_ml = df_ml.withColumn(\"label\", \n",
    "                         when(col(\"classificacao_risco\").isin(['CR√çTICO', 'ALTO']), 1)\n",
    "                         .otherwise(0))\n",
    "\n",
    "# Verificar distribui√ß√£o do target\n",
    "label_dist = df_ml.groupBy(\"label\").count().toPandas()\n",
    "print(\"\\nüìä Distribui√ß√£o do Target:\")\n",
    "print(label_dist)\n",
    "\n",
    "total = label_dist['count'].sum()\n",
    "for _, row in label_dist.iterrows():\n",
    "    perc = (row['count'] / total) * 100\n",
    "    classe = \"Alto Risco\" if row['label'] == 1 else \"Baixo/M√©dio Risco\"\n",
    "    print(f\"  ‚Ä¢ Classe {row['label']} ({classe}): {row['count']:,} ({perc:.1f}%)\")\n",
    "\n",
    "# Selecionar features para o modelo\n",
    "features_ml = [\n",
    "    'score_risco',\n",
    "    'saldo_credor_atual',\n",
    "    'qtde_ultimos_meses_iguais',\n",
    "    'crescimento_saldo_percentual',\n",
    "    'vl_credito_presumido_13m',\n",
    "    'desvio_padrao_credito',\n",
    "    'media_credito_13m',\n",
    "    'intensidade_bc',\n",
    "    'ratio_cresc_saldo',\n",
    "    'taxa_extremas',\n",
    "    'consistencia_periodo',\n",
    "    'volatilidade_credito'\n",
    "]\n",
    "\n",
    "# Tratar valores nulos\n",
    "df_ml = df_ml.fillna(0, subset=features_ml)\n",
    "\n",
    "# Assemblar features\n",
    "assembler = VectorAssembler(inputCols=features_ml, outputCol=\"features_unscaled\")\n",
    "df_ml = assembler.transform(df_ml)\n",
    "\n",
    "# Normalizar features\n",
    "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", \n",
    "                       withStd=True, withMean=False)\n",
    "scaler_model = scaler.fit(df_ml)\n",
    "ml_data = scaler_model.transform(df_ml)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset ML preparado com {len(features_ml)} features\")\n",
    "print(f\"   Total de registros: {ml_data.count():,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CLUSTERING K-MEANS - SEGMENTA√á√ÉO DE EMPRESAS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2Ô∏è‚É£ CLUSTERING K-MEANS - SEGMENTA√á√ÉO DE EMPRESAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Preparar dados para clustering (agregado por empresa)\n",
    "from pyspark.sql.functions import col as spark_col, avg as spark_avg, max as spark_max\n",
    "df_cluster = df_ml.groupBy(\"nu_cnpj\", \"nm_razao_social\").agg(\n",
    "    spark_avg(\"score_risco\").alias(\"score_medio\"),\n",
    "    spark_avg(\"saldo_credor_atual\").alias(\"saldo_medio\"),\n",
    "    spark_avg(\"qtde_ultimos_meses_iguais\").alias(\"meses_iguais_medio\"),\n",
    "    spark_avg(\"crescimento_saldo_percentual\").alias(\"crescimento_medio\"),\n",
    "    spark_max(\"classificacao_risco\").alias(\"pior_classificacao\")\n",
    ").fillna(0)\n",
    "\n",
    "# Features para clustering\n",
    "cluster_features = ['score_medio', 'saldo_medio', 'meses_iguais_medio', 'crescimento_medio']\n",
    "cluster_assembler = VectorAssembler(inputCols=cluster_features, outputCol=\"cluster_features_unscaled\")\n",
    "df_cluster = cluster_assembler.transform(df_cluster)\n",
    "\n",
    "cluster_scaler = StandardScaler(inputCol=\"cluster_features_unscaled\", outputCol=\"cluster_features\",\n",
    "                               withStd=True, withMean=False)\n",
    "df_cluster = cluster_scaler.fit(df_cluster).transform(df_cluster)\n",
    "\n",
    "# Treinar K-Means\n",
    "print(\"\\nüîÑ Treinando modelo K-Means (k=5)...\")\n",
    "kmeans = KMeans(featuresCol='cluster_features', k=5, seed=42, predictionCol=\"cluster\")\n",
    "kmeans_model = kmeans.fit(df_cluster)\n",
    "empresas_clustered = kmeans_model.transform(df_cluster)\n",
    "empresas_clustered.cache()\n",
    "\n",
    "# Analisar clusters\n",
    "cluster_summary = empresas_clustered.groupBy(\"cluster\").agg(\n",
    "    count(\"*\").alias(\"num_empresas\"),\n",
    "    avg(\"score_medio\").alias(\"score_medio_cluster\"),\n",
    "    avg(\"saldo_medio\").alias(\"saldo_medio_cluster\"),\n",
    "    avg(\"meses_iguais_medio\").alias(\"estagnacao_media\"),\n",
    "    avg(\"crescimento_medio\").alias(\"crescimento_medio_cluster\")\n",
    ").orderBy(desc(\"score_medio_cluster\")).toPandas()\n",
    "\n",
    "# Converter Decimal para float\n",
    "for col_name in ['score_medio_cluster', 'saldo_medio_cluster', 'estagnacao_media', 'crescimento_medio_cluster']:\n",
    "    cluster_summary[col_name] = cluster_summary[col_name].astype(float)\n",
    "\n",
    "print(\"\\nüìä Perfis dos Clusters Identificados:\\n\")\n",
    "perfis_cluster = [\n",
    "    \"üî¥ CR√çTICO - Alto risco e alto impacto\",\n",
    "    \"üü† ALTO RISCO - Comportamento suspeito\",\n",
    "    \"üü° RISCO MODERADO - Monitoramento necess√°rio\",\n",
    "    \"üü¢ BAIXO RISCO - Comportamento normal\",\n",
    "    \"üîµ EST√ÅVEL - Padr√£o regular\"\n",
    "]\n",
    "\n",
    "for idx, row in cluster_summary.iterrows():\n",
    "    perfil = perfis_cluster[idx] if idx < len(perfis_cluster) else f\"Cluster {row['cluster']}\"\n",
    "    print(f\"{perfil}\")\n",
    "    print(f\"  Cluster {row['cluster']}: {row['num_empresas']:,} empresas\")\n",
    "    print(f\"  ‚Ä¢ Score m√©dio: {row['score_medio_cluster']:.1f}\")\n",
    "    print(f\"  ‚Ä¢ Saldo m√©dio: R$ {row['saldo_medio_cluster']:,.2f}\")\n",
    "    print(f\"  ‚Ä¢ Estagna√ß√£o m√©dia: {row['estagnacao_media']:.1f} meses\")\n",
    "    print(f\"  ‚Ä¢ Crescimento m√©dio: {row['crescimento_medio_cluster']:.1f}%\\n\")\n",
    "\n",
    "# Visualiza√ß√£o dos clusters\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Distribui√ß√£o de Empresas por Cluster', \n",
    "                   'Score M√©dio vs Saldo M√©dio por Cluster')\n",
    ")\n",
    "\n",
    "# Gr√°fico 1: Barras\n",
    "fig.add_trace(\n",
    "    go.Bar(x=cluster_summary['cluster'].astype(str), \n",
    "           y=cluster_summary['num_empresas'],\n",
    "           marker_color=['#d62728', '#ff7f0e', '#ffdd70', '#90ee90', '#1f77b4'],\n",
    "           text=cluster_summary['num_empresas'],\n",
    "           textposition='outside'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Gr√°fico 2: Scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=cluster_summary['saldo_medio_cluster']/1e6, \n",
    "               y=cluster_summary['score_medio_cluster'],\n",
    "               mode='markers+text',\n",
    "               marker=dict(size=cluster_summary['num_empresas']/100,\n",
    "                         color=cluster_summary['cluster'],\n",
    "                         colorscale='Reds',\n",
    "                         showscale=True),\n",
    "               text=cluster_summary['cluster'].astype(str),\n",
    "               textposition='top center'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Cluster\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"N√∫mero de Empresas\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Saldo M√©dio (Milh√µes R$)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Score M√©dio\", row=1, col=2)\n",
    "fig.update_layout(height=500, showlegend=False, title_text=\"An√°lise de Clusters K-Means\")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MODELOS DE CLASSIFICA√á√ÉO - RANDOM FOREST E GRADIENT BOOSTING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3Ô∏è‚É£ MODELOS DE CLASSIFICA√á√ÉO SUPERVISIONADA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split treino/teste\n",
    "(train_data, test_data) = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(f\"\\nüìä Divis√£o dos dados:\")\n",
    "print(f\"  ‚Ä¢ Treino: {train_data.count():,} registros\")\n",
    "print(f\"  ‚Ä¢ Teste: {test_data.count():,} registros\")\n",
    "\n",
    "# Balanceamento de classes\n",
    "from pyspark.sql.functions import col as spark_col, when as spark_when\n",
    "balance_ratio = train_data.filter(spark_col('label') == 0).count() / train_data.count()\n",
    "train_data = train_data.withColumn(\"weight\", \n",
    "                                   spark_when(spark_col(\"label\") == 1, balance_ratio)\n",
    "                                   .otherwise(1 - balance_ratio))\n",
    "\n",
    "# Modelo 1: Random Forest\n",
    "print(\"\\nüå≤ Treinando Random Forest Classifier...\")\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    weightCol='weight',\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Modelo 2: Gradient Boosting Trees\n",
    "print(\"üå≥ Treinando Gradient Boosting Classifier...\")\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    maxIter=100,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "gbt_model = gbt.fit(train_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "# Avalia√ß√£o dos modelos\n",
    "evaluators = {\n",
    "    'AUC-ROC': BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\"),\n",
    "    'F1-Score': MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\"),\n",
    "    'Precis√£o': MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedPrecision\"),\n",
    "    'Recall': MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "}\n",
    "\n",
    "resultados = []\n",
    "for model_name, predictions in [('Random Forest', rf_predictions), ('Gradient Boosting', gbt_predictions)]:\n",
    "    metrics = {'Modelo': model_name}\n",
    "    for metric_name, evaluator in evaluators.items():\n",
    "        score = evaluator.evaluate(predictions)\n",
    "        metrics[metric_name] = score\n",
    "    resultados.append(metrics)\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "print(\"\\nüìà PERFORMANCE DOS MODELOS:\\n\")\n",
    "print(df_resultados.to_string(index=False))\n",
    "\n",
    "# Selecionar melhor modelo\n",
    "melhor_modelo_row = df_resultados.loc[df_resultados['F1-Score'].idxmax()]\n",
    "melhor_modelo_nome = melhor_modelo_row['Modelo']\n",
    "melhor_modelo = rf_model if melhor_modelo_nome == 'Random Forest' else gbt_model\n",
    "melhor_predictions = rf_predictions if melhor_modelo_nome == 'Random Forest' else gbt_predictions\n",
    "\n",
    "print(f\"\\nüèÜ Melhor Modelo: {melhor_modelo_nome}\")\n",
    "print(f\"   F1-Score: {melhor_modelo_row['F1-Score']:.3f}\")\n",
    "print(f\"   AUC-ROC: {melhor_modelo_row['AUC-ROC']:.3f}\")\n",
    "\n",
    "# Visualiza√ß√£o de m√©tricas\n",
    "fig = go.Figure()\n",
    "for metric in ['AUC-ROC', 'F1-Score', 'Precis√£o', 'Recall']:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=df_resultados['Modelo'],\n",
    "        y=df_resultados[metric],\n",
    "        text=df_resultados[metric].round(3),\n",
    "        textposition='outside'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Compara√ß√£o de Performance dos Modelos\",\n",
    "    xaxis_title=\"Modelo\",\n",
    "    yaxis_title=\"Score\",\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    yaxis=dict(range=[0, 1.1])\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. FEATURE IMPORTANCE E INTERPRETABILIDADE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4Ô∏è‚É£ AN√ÅLISE DE IMPORT√ÇNCIA DAS FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if hasattr(melhor_modelo, 'featureImportances'):\n",
    "    importances = melhor_modelo.featureImportances.toArray()\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': features_ml,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüéØ Top 10 Features mais Importantes ({melhor_modelo_nome}):\\n\")\n",
    "    for idx, row in feature_importance_df.head(10).iterrows():\n",
    "        print(f\"  {row['feature']:<35}: {row['importance']:.4f} {'‚ñà' * int(row['importance']*50)}\")\n",
    "    \n",
    "    # Gr√°fico de import√¢ncia\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=feature_importance_df.head(10)['importance'],\n",
    "        y=feature_importance_df.head(10)['feature'],\n",
    "        orientation='h',\n",
    "        marker_color='steelblue',  # Cor fixa ao inv√©s de colorscale\n",
    "        text=feature_importance_df.head(10)['importance'].round(4),\n",
    "        textposition='outside'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Top 10 Features - {melhor_modelo_nome}\",\n",
    "        xaxis_title=\"Import√¢ncia\",\n",
    "        yaxis_title=\"Feature\",\n",
    "        height=500\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SISTEMA DE ALERTAS E PREDI√á√ïES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5Ô∏è‚É£ SISTEMA DE ALERTAS AUTOMATIZADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Aplicar modelo em toda a base\n",
    "full_predictions = melhor_modelo.transform(ml_data)\n",
    "\n",
    "# Extrair probabilidade da classe positiva (risco alto)\n",
    "udf_extract_prob = udf(lambda v: float(v[1]), FloatType())\n",
    "full_predictions = full_predictions.withColumn(\"prob_risco_ml\", udf_extract_prob(col(\"probability\")))\n",
    "\n",
    "# Criar n√≠veis de alerta\n",
    "full_predictions = full_predictions.withColumn(\"nivel_alerta_ml\",\n",
    "    when(col(\"prob_risco_ml\") >= 0.9, 'EMERGENCIAL')\n",
    "    .when(col(\"prob_risco_ml\") >= 0.7, 'CR√çTICO')\n",
    "    .when(col(\"prob_risco_ml\") >= 0.5, 'ALTO')\n",
    "    .when(col(\"prob_risco_ml\") >= 0.3, 'M√âDIO')\n",
    "    .otherwise('BAIXO')\n",
    ")\n",
    "\n",
    "# An√°lise dos alertas\n",
    "alertas_ml = full_predictions.groupBy(\"nivel_alerta_ml\").agg(\n",
    "    count(\"*\").alias(\"casos\"),\n",
    "    sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    avg(\"prob_risco_ml\").alias(\"prob_media\")\n",
    ").orderBy(\n",
    "    when(col(\"nivel_alerta_ml\") == 'EMERGENCIAL', 1)\n",
    "    .when(col(\"nivel_alerta_ml\") == 'CR√çTICO', 2)\n",
    "    .when(col(\"nivel_alerta_ml\") == 'ALTO', 3)\n",
    "    .when(col(\"nivel_alerta_ml\") == 'M√âDIO', 4)\n",
    "    .otherwise(5)\n",
    ").toPandas()\n",
    "\n",
    "# Converter colunas\n",
    "for col_name in ['saldo_total']:\n",
    "    alertas_ml[col_name] = alertas_ml[col_name].astype(float)\n",
    "\n",
    "print(\"\\nüö® ALERTAS GERADOS PELO MODELO ML:\\n\")\n",
    "for _, row in alertas_ml.iterrows():\n",
    "    print(f\"{row['nivel_alerta_ml']:<15}: {row['casos']:>6,} casos | R$ {row['saldo_total']:>15,.2f} | Prob: {row['prob_media']:.2%}\")\n",
    "\n",
    "# Visualiza√ß√£o dos alertas\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Quantidade de Casos', 'Impacto Financeiro'),\n",
    "    specs=[[{'type':'bar'}, {'type':'bar'}]]\n",
    ")\n",
    "\n",
    "cores_alerta = {'EMERGENCIAL':'#8B0000', 'CR√çTICO':'#d62728', 'ALTO':'#ff7f0e', \n",
    "                'M√âDIO':'#ffdd70', 'BAIXO':'#1f77b4'}\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=alertas_ml['nivel_alerta_ml'], y=alertas_ml['casos'],\n",
    "           marker_color=[cores_alerta.get(x, 'gray') for x in alertas_ml['nivel_alerta_ml']],\n",
    "           text=alertas_ml['casos'], textposition='outside'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=alertas_ml['nivel_alerta_ml'], y=alertas_ml['saldo_total']/1e6,\n",
    "           marker_color=[cores_alerta.get(x, 'gray') for x in alertas_ml['nivel_alerta_ml']],\n",
    "           text=[f'R$ {x/1e6:.1f}M' for x in alertas_ml['saldo_total']], textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"N√≠vel de Alerta\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Quantidade\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"N√≠vel de Alerta\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"BC Total (Milh√µes R$)\", row=1, col=2)\n",
    "fig.update_layout(height=500, showlegend=False, title_text=\"Sistema de Alertas ML\")\n",
    "fig.show()\n",
    "\n",
    "# Salvar predi√ß√µes\n",
    "full_predictions.createOrReplaceTempView(\"dados_finais_com_predicao\")\n",
    "print(\"\\n‚úÖ View 'dados_finais_com_predicao' criada no Spark SQL\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PIPELINE DE MACHINE LEARNING CONCLU√çDO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä RESUMO EXECUTIVO:\")\n",
    "print(f\"  ‚Ä¢ Melhor modelo: {melhor_modelo_nome}\")\n",
    "print(f\"  ‚Ä¢ F1-Score: {melhor_modelo_row['F1-Score']:.3f}\")\n",
    "print(f\"  ‚Ä¢ AUC-ROC: {melhor_modelo_row['AUC-ROC']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Alertas Emergenciais: {alertas_ml[alertas_ml['nivel_alerta_ml']=='EMERGENCIAL']['casos'].values[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Alertas Cr√≠ticos: {alertas_ml[alertas_ml['nivel_alerta_ml']=='CR√çTICO']['casos'].values[0]:,}\")\n",
    "print(f\"\\nüéØ Pr√≥ximo passo: Execute o Notebook 04 para Dashboards Interativos\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Limpar cache\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()\n",
    "empresas_clustered.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e660f-7e4f-4b49-84c8-89edc2de0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "NOTEBOOK 04: DASHBOARDS INTERATIVOS - VISUALIZA√á√ÉO EXECUTIVA\n",
    "=============================================================================\n",
    "Dashboards completos com Plotly para an√°lise executiva e operacional\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä DASHBOARDS INTERATIVOS - AN√ÅLISE DE CR√âDITOS ICMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DASHBOARD EXECUTIVO - VIS√ÉO GERAL\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1Ô∏è‚É£ DASHBOARD EXECUTIVO - VIS√ÉO GERAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Carregar dados com predi√ß√µes\n",
    "try:\n",
    "    df_completo = spark.table(\"dados_finais_com_predicao\")\n",
    "    print(\"‚úÖ View 'dados_finais_com_predicao' encontrada\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è View 'dados_finais_com_predicao' n√£o encontrada\")\n",
    "    print(\"   Usando tabela principal 'teste.credito_dime_completo'\")\n",
    "    df_completo = spark.table(\"teste.credito_dime_completo\")\n",
    "    \n",
    "    # Criar colunas simuladas de ML\n",
    "    df_completo = df_completo.withColumn(\"nivel_alerta_ml\",\n",
    "        when(col(\"classificacao_risco\") == 'CR√çTICO', 'EMERGENCIAL')\n",
    "        .when(col(\"score_risco\") >= 70, 'CR√çTICO')\n",
    "        .when(col(\"score_risco\") >= 50, 'ALTO')\n",
    "        .when(col(\"score_risco\") >= 30, 'M√âDIO')\n",
    "        .otherwise('BAIXO')\n",
    "    ).withColumn(\"prob_risco_ml\",\n",
    "        col(\"score_risco\") / 100.0\n",
    "    )\n",
    "\n",
    "# KPIs principais\n",
    "kpis = df_completo.agg(\n",
    "    count(\"*\").alias(\"total_empresas\"),\n",
    "    sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    sum(when(col(\"classificacao_risco\").isin(['CR√çTICO','ALTO']), 1).otherwise(0)).alias(\"casos_prioritarios\"),\n",
    "    sum(when(col(\"nivel_alerta_ml\") == 'EMERGENCIAL', 1).otherwise(0)).alias(\"alertas_emergenciais\"),\n",
    "    sum(when(col(\"qtde_ultimos_meses_iguais\") >= 12, 1).otherwise(0)).alias(\"congelados_12m\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nüìä KPIs PRINCIPAIS:\")\n",
    "print(f\"  ‚Ä¢ Total de empresas monitoradas: {kpis['total_empresas']:,}\")\n",
    "print(f\"  ‚Ä¢ Saldo credor total: R$ {kpis['saldo_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Casos priorit√°rios: {kpis['casos_prioritarios']:,}\")\n",
    "print(f\"  ‚Ä¢ Alertas emergenciais (ML): {kpis['alertas_emergenciais']:,}\")\n",
    "print(f\"  ‚Ä¢ Empresas congeladas ‚â•12m: {kpis['congelados_12m']:,}\")\n",
    "\n",
    "# Dashboard executivo com 4 pain√©is\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Distribui√ß√£o por Classifica√ß√£o de Risco',\n",
    "        'Evolu√ß√£o do Saldo Credor por Risco',\n",
    "        'Top 10 GERFEs Cr√≠ticas',\n",
    "        'Status dos Alertas ML'\n",
    "    ),\n",
    "    specs=[\n",
    "        [{'type':'pie'}, {'type':'bar'}],\n",
    "        [{'type':'bar'}, {'type':'indicator'}]\n",
    "    ],\n",
    "    vertical_spacing=0.15,\n",
    "    horizontal_spacing=0.15\n",
    ")\n",
    "\n",
    "# Painel 1: Pizza - Distribui√ß√£o por risco\n",
    "dist_risco = df_completo.groupBy(\"classificacao_risco\").count().toPandas()\n",
    "cores_risco = {'CR√çTICO':'#d62728', 'ALTO':'#ff7f0e', 'M√âDIO':'#ffdd70', 'BAIXO':'#1f77b4'}\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=dist_risco['classificacao_risco'],\n",
    "           values=dist_risco['count'],\n",
    "           marker_colors=[cores_risco.get(x, 'gray') for x in dist_risco['classificacao_risco']],\n",
    "           textinfo='percent+label',\n",
    "           hole=0.4),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Painel 2: Barras - Saldo por risco\n",
    "saldo_risco = df_completo.groupBy(\"classificacao_risco\").agg(\n",
    "    sum(\"saldo_credor_atual\").alias(\"saldo\")\n",
    ").toPandas()\n",
    "saldo_risco['saldo'] = saldo_risco['saldo'].astype(float)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=saldo_risco['classificacao_risco'],\n",
    "           y=saldo_risco['saldo']/1e6,\n",
    "           marker_color=[cores_risco.get(x, 'gray') for x in saldo_risco['classificacao_risco']],\n",
    "           text=[f'R$ {x/1e6:.1f}M' for x in saldo_risco['saldo']],\n",
    "           textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Painel 3: Top 10 GERFEs\n",
    "gerfe_top = df_completo.filter(\n",
    "    col(\"classificacao_risco\").isin(['CR√çTICO','ALTO'])\n",
    ").groupBy(\"nm_gerfe\").count().orderBy(desc(\"count\")).limit(10).toPandas()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(y=gerfe_top['nm_gerfe'],\n",
    "           x=gerfe_top['count'],\n",
    "           orientation='h',\n",
    "           marker_color='indianred',\n",
    "           text=gerfe_top['count'],\n",
    "           textposition='outside'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Painel 4: Indicador - Taxa de risco\n",
    "taxa_risco = (kpis['casos_prioritarios'] / kpis['total_empresas']) * 100\n",
    "fig.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number+delta\",\n",
    "        value=taxa_risco,\n",
    "        title={'text': \"Taxa de Risco (%)\"},\n",
    "        delta={'reference': 20, 'increasing': {'color': \"red\"}, 'decreasing': {'color': \"green\"}},\n",
    "        gauge={\n",
    "            'axis': {'range': [None, 100]},\n",
    "            'bar': {'color': \"darkred\"},\n",
    "            'steps': [\n",
    "                {'range': [0, 20], 'color': \"lightgreen\"},\n",
    "                {'range': [20, 40], 'color': \"yellow\"},\n",
    "                {'range': [40, 100], 'color': \"red\"}\n",
    "            ],\n",
    "            'threshold': {\n",
    "                'line': {'color': \"black\", 'width': 4},\n",
    "                'thickness': 0.75,\n",
    "                'value': 40\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    showlegend=False,\n",
    "    title_text=\"<b>DASHBOARD EXECUTIVO - CR√âDITOS ICMS ACUMULADOS</b>\",\n",
    "    title_font_size=20\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DASHBOARD SETORIAL - AN√ÅLISE COMPARATIVA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2Ô∏è‚É£ DASHBOARD SETORIAL - COMPARATIVO T√äXTIL, METAL-MEC√ÇNICO E TECH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Carregar tabelas setoriais\n",
    "df_textil = spark.table(\"teste.credito_dime_textil\")\n",
    "df_metalmec = spark.table(\"teste.credito_dime_metalmec\")\n",
    "df_tech = spark.table(\"teste.credito_dime_tech\")\n",
    "\n",
    "# M√©tricas setoriais\n",
    "setores_data = []\n",
    "\n",
    "for nome, df, flag_col, entradas_col, saidas_col in [\n",
    "    ('T√äXTIL', df_textil, 'flag_setor_textil', 'vl_entradas_textil', 'vl_saidas_textil'),\n",
    "    ('METAL-MEC√ÇNICO', df_metalmec, 'flag_setor_metalmec', 'vl_entradas_metalmec', 'vl_saidas_metalmec'),\n",
    "    ('TECNOLOGIA', df_tech, 'flag_setor_tech', 'vl_entradas_tech', 'vl_saidas_tech')\n",
    "]:\n",
    "    stats = df.filter(col(flag_col) == 1).agg(\n",
    "        count(\"*\").alias(\"total\"),\n",
    "        sum(\"saldo_credor_atual\").alias(\"saldo\"),\n",
    "        sum(col(entradas_col)).alias(\"entradas\"),\n",
    "        sum(col(saidas_col)).alias(\"saidas\"),\n",
    "        sum(when(col(\"classificacao_risco\") == 'CR√çTICO', 1).otherwise(0)).alias(\"criticos\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    setores_data.append({\n",
    "        'Setor': nome,\n",
    "        'Empresas': stats['total'],\n",
    "        'Saldo Total': float(stats['saldo']),\n",
    "        'Entradas': float(stats['entradas']),\n",
    "        'Sa√≠das': float(stats['saidas']),\n",
    "        'Cr√≠ticos': stats['criticos']\n",
    "    })\n",
    "\n",
    "df_setores = pd.DataFrame(setores_data)\n",
    "\n",
    "print(\"\\nüìä Comparativo Setorial:\")\n",
    "print(df_setores.to_string(index=False))\n",
    "\n",
    "# Dashboard setorial\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Empresas por Setor',\n",
    "        'Saldo Credor por Setor',\n",
    "        'Movimenta√ß√£o Setorial (Entradas vs Sa√≠das)',\n",
    "        'Casos Cr√≠ticos por Setor'\n",
    "    ),\n",
    "    specs=[\n",
    "        [{'type':'bar'}, {'type':'bar'}],\n",
    "        [{'type':'bar'}, {'type':'bar'}]\n",
    "    ]\n",
    ")\n",
    "\n",
    "cores_setores = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# Empresas por setor\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_setores['Setor'], y=df_setores['Empresas'],\n",
    "           marker_color=cores_setores,\n",
    "           text=df_setores['Empresas'], textposition='outside'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Saldo por setor\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_setores['Setor'], y=df_setores['Saldo Total']/1e6,\n",
    "           marker_color=cores_setores,\n",
    "           text=[f'R$ {x/1e6:.1f}M' for x in df_setores['Saldo Total']],\n",
    "           textposition='outside'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Movimenta√ß√£o setorial (barras agrupadas)\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Entradas', x=df_setores['Setor'], y=df_setores['Entradas']/1e6,\n",
    "           marker_color='lightblue'),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Sa√≠das', x=df_setores['Setor'], y=df_setores['Sa√≠das']/1e6,\n",
    "           marker_color='salmon'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Casos cr√≠ticos\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_setores['Setor'], y=df_setores['Cr√≠ticos'],\n",
    "           marker_color='darkred',\n",
    "           text=df_setores['Cr√≠ticos'], textposition='outside'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Setor\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Quantidade\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Setor\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Saldo (Milh√µes R$)\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Setor\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Valor (Milh√µes R$)\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Setor\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Casos Cr√≠ticos\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"<b>DASHBOARD SETORIAL - AN√ÅLISE COMPARATIVA</b>\",\n",
    "    title_font_size=20,\n",
    "    barmode='group'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MAPA DE CALOR - RISCO POR GERFE E PER√çODO\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3Ô∏è‚É£ MAPA DE CALOR - DISTRIBUI√á√ÉO GEOGR√ÅFICA DO RISCO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Criar matriz GERFE x Score M√©dio\n",
    "gerfe_score = df_completo.groupBy(\"nm_gerfe\").agg(\n",
    "    avg(\"score_risco\").alias(\"score_medio\"),\n",
    "    count(\"*\").alias(\"total_empresas\"),\n",
    "    sum(when(col(\"classificacao_risco\") == 'CR√çTICO', 1).otherwise(0)).alias(\"criticos\")\n",
    ").orderBy(desc(\"score_medio\")).limit(20).toPandas()\n",
    "\n",
    "# Criar mapa de calor\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    y=gerfe_score['nm_gerfe'],\n",
    "    z=[gerfe_score['score_medio'].values],\n",
    "    x=['Score M√©dio de Risco'],\n",
    "    colorscale='Reds',\n",
    "    text=[[f\"Score: {s:.1f}<br>Empresas: {e}<br>Cr√≠ticos: {c}\" \n",
    "           for s, e, c in zip(gerfe_score['score_medio'], \n",
    "                             gerfe_score['total_empresas'], \n",
    "                             gerfe_score['criticos'])]],\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 10},\n",
    "    colorbar=dict(title=\"Score<br>Risco\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>MAPA DE CALOR - TOP 20 GERFEs POR RISCO</b>\",\n",
    "    height=700,\n",
    "    xaxis_title=\"\",\n",
    "    yaxis_title=\"GERFE\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. AN√ÅLISE TEMPORAL INTERATIVA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4Ô∏è‚É£ AN√ÅLISE TEMPORAL - EVOLU√á√ÉO DOS INDICADORES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simular evolu√ß√£o temporal (√∫ltimos 13 meses)\n",
    "periodos = ['202409', '202410', '202411', '202412', '202501', '202502', '202503', \n",
    "            '202504', '202505', '202506', '202507', '202508', '202509']\n",
    "\n",
    "# Criar dados temporais simulados com base nas caracter√≠sticas atuais\n",
    "temporal_data = []\n",
    "for i, periodo in enumerate(periodos):\n",
    "    # Simular crescimento gradual dos casos cr√≠ticos\n",
    "    fator = 1 + (i * 0.05)\n",
    "    casos_criticos = int(kpis['casos_prioritarios'] * (0.7 + i * 0.02))\n",
    "    saldo = float(kpis['saldo_total']) * fator  # Converter para float antes de multiplicar\n",
    "    \n",
    "    temporal_data.append({\n",
    "        'Periodo': periodo,\n",
    "        'Data': pd.to_datetime(periodo, format='%Y%m'),\n",
    "        'Casos_Criticos': casos_criticos,\n",
    "        'Saldo_Total': saldo,\n",
    "        'Score_Medio': 45 + i * 1.5,\n",
    "        'Congelados': int(kpis['congelados_12m'] * (0.8 + i * 0.015))\n",
    "    })\n",
    "\n",
    "df_temporal = pd.DataFrame(temporal_data)\n",
    "\n",
    "# Gr√°fico temporal interativo com m√∫ltiplos eixos\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=('Evolu√ß√£o de Casos Cr√≠ticos e Score M√©dio', \n",
    "                   'Evolu√ß√£o do Saldo Credor Total'),\n",
    "    vertical_spacing=0.15,\n",
    "    specs=[[{\"secondary_y\": True}], [{\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# Linha 1: Casos cr√≠ticos\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_temporal['Data'], y=df_temporal['Casos_Criticos'],\n",
    "               mode='lines+markers',\n",
    "               name='Casos Cr√≠ticos',\n",
    "               line=dict(color='red', width=3),\n",
    "               marker=dict(size=8)),\n",
    "    row=1, col=1, secondary_y=False\n",
    ")\n",
    "\n",
    "# Linha 2: Score m√©dio\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_temporal['Data'], y=df_temporal['Score_Medio'],\n",
    "               mode='lines+markers',\n",
    "               name='Score M√©dio',\n",
    "               line=dict(color='orange', width=2, dash='dash'),\n",
    "               marker=dict(size=6)),\n",
    "    row=1, col=1, secondary_y=True\n",
    ")\n",
    "\n",
    "# √Årea: Saldo total\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_temporal['Data'], y=df_temporal['Saldo_Total']/1e9,\n",
    "               mode='lines',\n",
    "               name='Saldo Credor',\n",
    "               fill='tozeroy',\n",
    "               line=dict(color='green', width=2)),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Per√≠odo\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Per√≠odo\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Casos Cr√≠ticos\", row=1, col=1, secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Score M√©dio\", row=1, col=1, secondary_y=True)\n",
    "fig.update_yaxes(title_text=\"Saldo (Bilh√µes R$)\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"<b>AN√ÅLISE TEMPORAL - EVOLU√á√ÉO DOS INDICADORES</b>\",\n",
    "    title_font_size=20,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SCATTER 3D - AN√ÅLISE MULTIDIMENSIONAL\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5Ô∏è‚É£ VISUALIZA√á√ÉO 3D - AN√ÅLISE MULTIDIMENSIONAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Amostra para visualiza√ß√£o 3D\n",
    "amostra_3d = df_completo.filter(\n",
    "    col(\"classificacao_risco\").isin(['CR√çTICO', 'ALTO'])\n",
    ").sample(False, 0.1).limit(500).select(\n",
    "    \"nm_razao_social\",\n",
    "    \"score_risco\",\n",
    "    \"saldo_credor_atual\",\n",
    "    \"qtde_ultimos_meses_iguais\",\n",
    "    \"crescimento_saldo_percentual\",\n",
    "    \"classificacao_risco\",\n",
    "    \"nivel_alerta_ml\"\n",
    ").toPandas()\n",
    "\n",
    "# Converter Decimal para float\n",
    "amostra_3d['saldo_credor_atual'] = amostra_3d['saldo_credor_atual'].astype(float)\n",
    "amostra_3d['crescimento_saldo_percentual'] = amostra_3d['crescimento_saldo_percentual'].astype(float)\n",
    "\n",
    "# Scatter 3D\n",
    "fig = px.scatter_3d(\n",
    "    amostra_3d,\n",
    "    x='score_risco',\n",
    "    y='saldo_credor_atual',\n",
    "    z='crescimento_saldo_percentual',\n",
    "    color='classificacao_risco',\n",
    "    size='qtde_ultimos_meses_iguais',\n",
    "    hover_name='nm_razao_social',\n",
    "    hover_data=['nivel_alerta_ml'],\n",
    "    color_discrete_map={'CR√çTICO': '#d62728', 'ALTO': '#ff7f0e'},\n",
    "    title='<b>AN√ÅLISE 3D: Score √ó Saldo √ó Crescimento</b>',\n",
    "    labels={\n",
    "        'score_risco': 'Score de Risco',\n",
    "        'saldo_credor_atual': 'Saldo Credor (R$)',\n",
    "        'crescimento_saldo_percentual': 'Crescimento (%)',\n",
    "        'qtde_ultimos_meses_iguais': 'Meses Estagnados'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700)\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. RANKING INTERATIVO - TOP EMPRESAS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6Ô∏è‚É£ RANKING INTERATIVO - TOP 50 EMPRESAS CR√çTICAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_50 = df_completo.filter(\n",
    "    col(\"classificacao_risco\").isin(['CR√çTICO', 'ALTO'])\n",
    ").orderBy(\n",
    "    desc(\"score_risco\"),\n",
    "    desc(\"saldo_credor_atual\")\n",
    ").limit(50).select(\n",
    "    \"nu_cnpj\",\n",
    "    \"nm_razao_social\",\n",
    "    \"nm_gerfe\",\n",
    "    \"score_risco\",\n",
    "    \"classificacao_risco\",\n",
    "    \"saldo_credor_atual\",\n",
    "    \"qtde_ultimos_meses_iguais\",\n",
    "    \"crescimento_saldo_percentual\",\n",
    "    \"nivel_alerta_ml\",\n",
    "    \"prob_risco_ml\"\n",
    ").toPandas()\n",
    "\n",
    "# Converter colunas\n",
    "for col in ['saldo_credor_atual', 'crescimento_saldo_percentual', 'prob_risco_ml']:\n",
    "    top_50[col] = top_50[col].astype(float)\n",
    "\n",
    "# Adicionar ranking\n",
    "top_50['Ranking'] = range(1, len(top_50) + 1)\n",
    "\n",
    "# Criar tabela interativa\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=['<b>Rank</b>', '<b>CNPJ</b>', '<b>Raz√£o Social</b>', '<b>GERFE</b>', \n",
    "                '<b>Score</b>', '<b>Risco</b>', '<b>Saldo (R$)</b>', \n",
    "                '<b>Meses Iguais</b>', '<b>Cresc. %</b>', '<b>Alerta ML</b>', '<b>Prob. ML</b>'],\n",
    "        fill_color='darkred',\n",
    "        font=dict(color='white', size=12),\n",
    "        align='center'\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[\n",
    "            top_50['Ranking'],\n",
    "            top_50['nu_cnpj'],\n",
    "            top_50['nm_razao_social'].str[:40],\n",
    "            top_50['nm_gerfe'],\n",
    "            top_50['score_risco'].round(0),\n",
    "            top_50['classificacao_risco'],\n",
    "            ['R$ {:,.2f}'.format(x) for x in top_50['saldo_credor_atual']],\n",
    "            top_50['qtde_ultimos_meses_iguais'],\n",
    "            ['{:+.1f}%'.format(x) for x in top_50['crescimento_saldo_percentual']],\n",
    "            top_50['nivel_alerta_ml'],\n",
    "            ['{:.1%}'.format(x) for x in top_50['prob_risco_ml']]\n",
    "        ],\n",
    "        fill_color=[['white', 'lightgray'] * 25],\n",
    "        font=dict(size=11),\n",
    "        align=['center', 'left', 'left', 'left', 'center', 'center', \n",
    "               'right', 'center', 'right', 'center', 'center']\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>TOP 50 EMPRESAS PRIORIT√ÅRIAS PARA FISCALIZA√á√ÉO</b>\",\n",
    "    title_font_size=18,\n",
    "    height=1000\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. DASHBOARD DE ALERTAS ML\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7Ô∏è‚É£ DASHBOARD DE ALERTAS - SISTEMA ML\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# An√°lise detalhada dos alertas ML\n",
    "alertas_detalhado = df_completo.groupBy(\"nivel_alerta_ml\", \"classificacao_risco\").agg(\n",
    "    count(\"*\").alias(\"casos\"),\n",
    "    sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    avg(\"prob_risco_ml\").alias(\"prob_media\")\n",
    ").toPandas()\n",
    "\n",
    "alertas_detalhado['saldo_total'] = alertas_detalhado['saldo_total'].astype(float)\n",
    "\n",
    "# Matriz de confus√£o: Alerta ML vs Classifica√ß√£o Original\n",
    "matriz_alerta = alertas_detalhado.pivot(\n",
    "    index='nivel_alerta_ml',\n",
    "    columns='classificacao_risco',\n",
    "    values='casos'\n",
    ").fillna(0)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=matriz_alerta.values,\n",
    "    x=matriz_alerta.columns,\n",
    "    y=matriz_alerta.index,\n",
    "    colorscale='Reds',\n",
    "    text=matriz_alerta.values.astype(int),\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 14},\n",
    "    colorbar=dict(title=\"Casos\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>MATRIZ: ALERTA ML √ó CLASSIFICA√á√ÉO ORIGINAL</b>\",\n",
    "    xaxis_title=\"Classifica√ß√£o de Risco Original\",\n",
    "    yaxis_title=\"N√≠vel de Alerta ML\",\n",
    "    height=600\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 8. SUNBURST - HIERARQUIA DE RISCOS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"8Ô∏è‚É£ VISUALIZA√á√ÉO SUNBURST - HIERARQUIA DE RISCOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Preparar dados hier√°rquicos\n",
    "hierarquia = df_completo.groupBy(\"classificacao_risco\", \"nivel_alerta_ml\").agg(\n",
    "    count(\"*\").alias(\"casos\"),\n",
    "    sum(\"saldo_credor_atual\").alias(\"valor\")\n",
    ").toPandas()\n",
    "\n",
    "hierarquia['valor'] = hierarquia['valor'].astype(float)\n",
    "\n",
    "# Adicionar n√≠vel raiz\n",
    "hierarquia_root = pd.DataFrame([{\n",
    "    'classificacao_risco': '',\n",
    "    'nivel_alerta_ml': 'Total',\n",
    "    'casos': hierarquia['casos'].sum(),\n",
    "    'valor': hierarquia['valor'].sum()\n",
    "}])\n",
    "\n",
    "hierarquia_completa = pd.concat([hierarquia_root, hierarquia], ignore_index=True)\n",
    "\n",
    "# Criar colunas para sunburst\n",
    "hierarquia_completa['labels'] = hierarquia_completa.apply(\n",
    "    lambda x: x['nivel_alerta_ml'] if x['classificacao_risco'] == '' \n",
    "    else f\"{x['nivel_alerta_ml']}<br>{x['casos']} casos\", axis=1\n",
    ")\n",
    "\n",
    "hierarquia_completa['parents'] = hierarquia_completa.apply(\n",
    "    lambda x: '' if x['classificacao_risco'] == '' \n",
    "    else x['classificacao_risco'], axis=1\n",
    ")\n",
    "\n",
    "fig = go.Figure(go.Sunburst(\n",
    "    labels=hierarquia_completa['labels'],\n",
    "    parents=hierarquia_completa['parents'],\n",
    "    values=hierarquia_completa['valor'],\n",
    "    branchvalues=\"total\",\n",
    "    marker=dict(\n",
    "        colors=hierarquia_completa['casos'],\n",
    "        colorscale='Reds',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Casos\")\n",
    "    ),\n",
    "    text=hierarquia_completa['casos'],\n",
    "    hovertemplate='<b>%{label}</b><br>Valor: R$ %{value:,.0f}<br>Casos: %{text}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>HIERARQUIA: CLASSIFICA√á√ÉO √ó ALERTA ML √ó VALOR</b>\",\n",
    "    height=700\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 9. RESUMO EXECUTIVO E EXPORTA√á√ÉO\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã RESUMO EXECUTIVO - DASHBOARDS INTERATIVOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                      RESUMO EXECUTIVO - AN√ÅLISE COMPLETA                     ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üìä INDICADORES GERAIS:\n",
    "   ‚Ä¢ Total de empresas monitoradas: {kpis['total_empresas']:,}\n",
    "   ‚Ä¢ Saldo credor acumulado: R$ {kpis['saldo_total']:,.2f}\n",
    "   ‚Ä¢ Taxa de casos priorit√°rios: {(kpis['casos_prioritarios']/kpis['total_empresas']*100):.1f}%\n",
    "\n",
    "üö® ALERTAS CR√çTICOS:\n",
    "   ‚Ä¢ Casos priorit√°rios (ALTO + CR√çTICO): {kpis['casos_prioritarios']:,}\n",
    "   ‚Ä¢ Alertas emergenciais (ML): {kpis['alertas_emergenciais']:,}\n",
    "   ‚Ä¢ Empresas congeladas ‚â•12 meses: {kpis['congelados_12m']:,}\n",
    "\n",
    "üéØ SETORES CR√çTICOS:\n",
    "   ‚Ä¢ T√™xtil: {df_setores[df_setores['Setor']=='T√äXTIL']['Cr√≠ticos'].values[0]:,} casos cr√≠ticos\n",
    "   ‚Ä¢ Metal-Mec√¢nico: {df_setores[df_setores['Setor']=='METAL-MEC√ÇNICO']['Cr√≠ticos'].values[0]:,} casos cr√≠ticos\n",
    "   ‚Ä¢ Tecnologia: {df_setores[df_setores['Setor']=='TECNOLOGIA']['Cr√≠ticos'].values[0]:,} casos cr√≠ticos\n",
    "\n",
    "üí∞ IMPACTO FINANCEIRO:\n",
    "   ‚Ä¢ Saldo em risco CR√çTICO: R$ {float(saldo_risco[saldo_risco['classificacao_risco']=='CR√çTICO']['saldo'].values[0]):,.2f}\n",
    "   ‚Ä¢ Saldo em risco ALTO: R$ {float(saldo_risco[saldo_risco['classificacao_risco']=='ALTO']['saldo'].values[0]):,.2f}\n",
    "\n",
    "üìà RECOMENDA√á√ïES:\n",
    "   1. Fiscalizar IMEDIATAMENTE as {kpis['alertas_emergenciais']:,} empresas com alerta EMERGENCIAL\n",
    "   2. Auditar as {top_50.shape[0]} empresas do ranking priorit√°rio\n",
    "   3. Investigar as {kpis['congelados_12m']:,} empresas com valores congelados\n",
    "   4. Monitorar clusters de alto risco identificados pelo ML\n",
    "\n",
    "üîó DADOS DISPON√çVEIS:\n",
    "   ‚Ä¢ View Spark: 'dados_finais_com_predicao'\n",
    "   ‚Ä¢ Tabelas setoriais: credito_dime_textil, credito_dime_metalmec, credito_dime_tech\n",
    "   ‚Ä¢ Rankings e alertas prontos para exporta√ß√£o\n",
    "\"\"\")\n",
    "\n",
    "# Exportar lista priorit√°ria para CSV (opcional)\n",
    "print(\"\\nüíæ Exportando lista priorit√°ria...\")\n",
    "arquivo_export = \"lista_prioritaria_fiscalizacao.csv\"\n",
    "top_50.to_csv(arquivo_export, index=False, encoding='utf-8-sig', sep=';')\n",
    "print(f\"‚úÖ Arquivo exportado: {arquivo_export}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TODOS OS DASHBOARDS CONCLU√çDOS COM SUCESSO!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "üìä DASHBOARDS CRIADOS:\n",
    "   1. ‚úì Dashboard Executivo - Vis√£o Geral\n",
    "   2. ‚úì Dashboard Setorial - An√°lise Comparativa\n",
    "   3. ‚úì Mapa de Calor - Distribui√ß√£o Geogr√°fica\n",
    "   4. ‚úì An√°lise Temporal - Evolu√ß√£o de Indicadores\n",
    "   5. ‚úì Visualiza√ß√£o 3D - An√°lise Multidimensional\n",
    "   6. ‚úì Ranking Interativo - Top 50 Empresas\n",
    "   7. ‚úì Dashboard de Alertas ML\n",
    "   8. ‚úì Sunburst - Hierarquia de Riscos\n",
    "\n",
    "üéØ SISTEMA COMPLETO OPERACIONAL\n",
    "   Todos os notebooks est√£o prontos para uso em produ√ß√£o!\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c1134-fb68-4917-823c-379ff2062805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "NOTEBOOK 05: AN√ÅLISE SETORIAL DETALHADA - T√äXTIL, METAL-MEC√ÇNICO E TECH\n",
    "=============================================================================\n",
    "An√°lise profunda dos padr√µes de movimenta√ß√£o e irregularidades por setor\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# Imports com aliases para evitar conflitos\n",
    "from pyspark.sql.functions import (\n",
    "    col as spark_col, when as spark_when, desc as spark_desc, \n",
    "    asc as spark_asc, lit as spark_lit, sum as spark_sum,\n",
    "    avg as spark_avg, count as spark_count, max as spark_max,\n",
    "    min as spark_min, countDistinct\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üè≠ AN√ÅLISE SETORIAL DETALHADA - PADR√ïES E IRREGULARIDADES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SETOR T√äXTIL - AN√ÅLISE COMPLETA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1Ô∏è‚É£ SETOR T√äXTIL (NCM 61xx e 62xx)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_textil = spark.table(\"teste.credito_dime_textil\").filter(spark_col(\"flag_setor_textil\") == 1)\n",
    "df_textil.cache()\n",
    "\n",
    "# Estat√≠sticas gerais\n",
    "stats_textil = df_textil.agg(\n",
    "    spark_count(\"*\").alias(\"total\"),\n",
    "    spark_sum(\"vl_entradas_textil\").alias(\"entradas_total\"),\n",
    "    spark_sum(\"vl_saidas_textil\").alias(\"saidas_total\"),\n",
    "    spark_sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    spark_avg(\"indice_entrada_saida_textil\").alias(\"indice_es_medio\"),\n",
    "    countDistinct(\"qtde_ncm_distintos_textil\").alias(\"diversidade_ncm\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nüìä PANORAMA SETOR T√äXTIL:\")\n",
    "print(f\"  ‚Ä¢ Empresas operando: {stats_textil['total']:,}\")\n",
    "print(f\"  ‚Ä¢ Entradas totais: R$ {stats_textil['entradas_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Sa√≠das totais: R$ {stats_textil['saidas_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Saldo credor: R$ {stats_textil['saldo_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ √çndice E/S m√©dio: {stats_textil['indice_es_medio']:.2f}\")\n",
    "\n",
    "# Padr√µes de movimenta√ß√£o suspeitos\n",
    "padroes_textil = df_textil.withColumn(\"padrao_movimento\",\n",
    "    spark_when((spark_col(\"vl_saidas_textil\") == 0) & (spark_col(\"vl_entradas_textil\") > 10000), \"SUSPEITO: S√≥ compra\")\n",
    "    .when(spark_col(\"vl_entradas_textil\") > spark_col(\"vl_saidas_textil\") * 3, \"ALERTA: Compra 3x+ que vende\")\n",
    "    .when(spark_col(\"vl_saidas_textil\") > spark_col(\"vl_entradas_textil\") * 3, \"ATEN√á√ÉO: Vende 3x+ que compra (poss√≠vel ST)\")\n",
    "    .otherwise(\"Normal\")\n",
    ").groupBy(\"padrao_movimento\", \"classificacao_risco\").agg(\n",
    "    spark_count(\"*\").alias(\"casos\"),\n",
    "    spark_sum(\"saldo_credor_atual\").alias(\"saldo_total\")\n",
    ").toPandas()\n",
    "\n",
    "padroes_textil['saldo_total'] = padroes_textil['saldo_total'].astype(float)\n",
    "\n",
    "print(\"\\nüîç PADR√ïES DE MOVIMENTA√á√ÉO T√äXTIL:\")\n",
    "for _, row in padroes_textil.sort_values('saldo_total', ascending=False).iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['padrao_movimento']:<50}\")\n",
    "    print(f\"    Casos: {row['casos']:>5,} | Risco: {row['classificacao_risco']:<10} | Saldo: R$ {row['saldo_total']:>15,.2f}\")\n",
    "\n",
    "# Top 20 empresas t√™xteis cr√≠ticas\n",
    "top_textil = df_textil.filter(\n",
    "    spark_col(\"classificacao_risco\").isin(['CR√çTICO', 'ALTO'])\n",
    ").orderBy(\n",
    "    spark_desc(\"score_risco\"),\n",
    "    spark_desc(\"saldo_credor_atual\")\n",
    ").limit(20).select(\n",
    "    \"nu_cnpj\",\n",
    "    \"nm_razao_social\",\n",
    "    \"saldo_credor_atual\",\n",
    "    \"vl_entradas_textil\",\n",
    "    \"vl_saidas_textil\",\n",
    "    \"dias_movimento_textil\",\n",
    "    \"qtde_ncm_distintos_textil\",\n",
    "    \"indice_entrada_saida_textil\",\n",
    "    \"score_risco\",\n",
    "    \"classificacao_risco\"\n",
    ").toPandas()\n",
    "\n",
    "# Converter colunas\n",
    "for col_name in ['saldo_credor_atual', 'vl_entradas_textil', 'vl_saidas_textil', 'indice_entrada_saida_textil']:\n",
    "    top_textil[col_name] = top_textil[col_name].astype(float)\n",
    "\n",
    "print(f\"\\nüéØ TOP 20 EMPRESAS T√äXTEIS CR√çTICAS:\\n\")\n",
    "for idx, row in top_textil.head(20).iterrows():\n",
    "    print(f\"{idx+1:2d}. {row['nm_razao_social'][:50]:<50}\")\n",
    "    print(f\"    Saldo: R$ {row['saldo_credor_atual']:>12,.2f} | Score: {row['score_risco']:>5.0f}\")\n",
    "    print(f\"    Entradas: R$ {row['vl_entradas_textil']:>12,.2f} | Sa√≠das: R$ {row['vl_saidas_textil']:>12,.2f}\")\n",
    "    print(f\"    √çndice E/S: {row['indice_entrada_saida_textil']:>6.2f} | NCMs: {row['qtde_ncm_distintos_textil']:>3}\\n\")\n",
    "\n",
    "# Visualiza√ß√£o T√™xtil\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Padr√µes de Movimenta√ß√£o',\n",
    "        'Dispers√£o: Entradas vs Sa√≠das (Top 100)',\n",
    "        'Distribui√ß√£o de Score por Padr√£o',\n",
    "        'Saldo Credor por Padr√£o'\n",
    "    ),\n",
    "    specs=[[{'type':'bar'}, {'type':'scatter'}],\n",
    "           [{'type':'box'}, {'type':'bar'}]]\n",
    ")\n",
    "\n",
    "# Painel 1: Barras - Padr√µes\n",
    "padroes_agg = padroes_textil.groupby('padrao_movimento')['casos'].sum().reset_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=padroes_agg['padrao_movimento'], y=padroes_agg['casos'],\n",
    "           marker_color='indianred'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Painel 2: Scatter - Entradas vs Sa√≠das\n",
    "from pyspark.sql.functions import col as spark_col, desc as spark_desc\n",
    "scatter_data = df_textil.filter(\n",
    "    spark_col(\"vl_entradas_textil\") > 0\n",
    ").orderBy(spark_desc(\"score_risco\")).limit(100).toPandas()\n",
    "scatter_data['vl_entradas_textil'] = scatter_data['vl_entradas_textil'].astype(float)\n",
    "scatter_data['vl_saidas_textil'] = scatter_data['vl_saidas_textil'].astype(float)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=scatter_data['vl_entradas_textil']/1e6,\n",
    "               y=scatter_data['vl_saidas_textil']/1e6,\n",
    "               mode='markers',\n",
    "               marker=dict(size=8, color=scatter_data['score_risco'], \n",
    "                         colorscale='Reds', showscale=True),\n",
    "               text=scatter_data['nm_razao_social'],\n",
    "               hovertemplate='%{text}<br>Entradas: R$ %{x}M<br>Sa√≠das: R$ %{y}M'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Linha de equil√≠brio (y=x)\n",
    "max_entradas = float(scatter_data['vl_entradas_textil'].max())\n",
    "max_saidas = float(scatter_data['vl_saidas_textil'].max())\n",
    "max_val = np.maximum(max_entradas, max_saidas) / 1e6  # Usar numpy.maximum ao inv√©s de max()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[0, max_val], y=[0, max_val],\n",
    "               mode='lines',\n",
    "               line=dict(dash='dash', color='gray'),\n",
    "               showlegend=False,\n",
    "               hoverinfo='skip'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Painel 3: Box plot - Score por padr√£o\n",
    "from pyspark.sql.functions import col as spark_col, lit as spark_lit\n",
    "for padrao in padroes_textil['padrao_movimento'].unique():\n",
    "    if \"S√≥ compra\" in padrao:\n",
    "        filtro = (spark_col(\"vl_saidas_textil\") == 0) & (spark_col(\"vl_entradas_textil\") > 10000)\n",
    "    elif \"3x+\" in padrao and \"compra\" in padrao.lower():\n",
    "        filtro = spark_col(\"vl_entradas_textil\") > spark_col(\"vl_saidas_textil\") * 3\n",
    "    elif \"3x+\" in padrao:\n",
    "        filtro = spark_col(\"vl_saidas_textil\") > spark_col(\"vl_entradas_textil\") * 3\n",
    "    else:\n",
    "        filtro = spark_lit(True)\n",
    "    \n",
    "    scores = df_textil.filter(filtro).select(\"score_risco\").toPandas()['score_risco'].values\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Box(y=scores, name=padrao[:20], showlegend=False),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Painel 4: Saldo por padr√£o\n",
    "padroes_saldo = padroes_textil.groupby('padrao_movimento')['saldo_total'].sum().reset_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=padroes_saldo['padrao_movimento'], y=padroes_saldo['saldo_total']/1e6,\n",
    "           marker_color='darkred',\n",
    "           text=[f'R$ {x/1e6:.1f}M' for x in padroes_saldo['saldo_total']],\n",
    "           textposition='outside'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Padr√£o\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Casos\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Entradas (Milh√µes R$)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Sa√≠das (Milh√µes R$)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Score de Risco\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Padr√£o\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Saldo (Milh√µes R$)\", row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=900, showlegend=False, \n",
    "                  title_text=\"<b>AN√ÅLISE DETALHADA - SETOR T√äXTIL</b>\")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 2. SETOR METAL-MEC√ÇNICO - AN√ÅLISE COMPLETA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2Ô∏è‚É£ SETOR METAL-MEC√ÇNICO (NCM 84xx e 85xx)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pyspark.sql.functions import col as spark_col\n",
    "df_metalmec = spark.table(\"teste.credito_dime_metalmec\").filter(spark_col(\"flag_setor_metalmec\") == 1)\n",
    "df_metalmec.cache()\n",
    "\n",
    "# Estat√≠sticas gerais\n",
    "stats_mm = df_metalmec.agg(\n",
    "    spark_count(\"*\").alias(\"total\"),\n",
    "    spark_sum(\"vl_entradas_metalmec\").alias(\"entradas_total\"),\n",
    "    spark_sum(\"vl_saidas_metalmec\").alias(\"saidas_total\"),\n",
    "    spark_sum(\"vl_capitulo_84\").alias(\"cap84_total\"),\n",
    "    spark_sum(\"vl_capitulo_85\").alias(\"cap85_total\"),\n",
    "    spark_avg(\"perc_cap84\").alias(\"perc_cap84_media\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nüìä PANORAMA SETOR METAL-MEC√ÇNICO:\")\n",
    "print(f\"  ‚Ä¢ Empresas operando: {stats_mm['total']:,}\")\n",
    "print(f\"  ‚Ä¢ Entradas totais: R$ {stats_mm['entradas_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Sa√≠das totais: R$ {stats_mm['saidas_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Cap. 84 (M√°quinas): R$ {stats_mm['cap84_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Cap. 85 (El√©tricos): R$ {stats_mm['cap85_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ % m√©dio Cap 84: {stats_mm['perc_cap84_media']:.1f}%\")\n",
    "\n",
    "# An√°lise Cap 84 vs Cap 85\n",
    "perfil_produto = df_metalmec.withColumn(\"perfil\",\n",
    "    spark_when(spark_col(\"perc_cap84\") > 80, \"Predominante CAP 84 (M√°quinas)\")\n",
    "    .when(spark_col(\"perc_cap84\") < 20, \"Predominante CAP 85 (El√©tricos)\")\n",
    "    .otherwise(\"Misto\")\n",
    ").groupBy(\"perfil\", \"classificacao_risco\").agg(\n",
    "    spark_count(\"*\").alias(\"empresas\"),\n",
    "    spark_sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    spark_avg(\"vl_capitulo_84\").alias(\"media_cap84\"),\n",
    "    spark_avg(\"vl_capitulo_85\").alias(\"media_cap85\")\n",
    ").toPandas()\n",
    "\n",
    "perfil_produto['saldo_total'] = perfil_produto['saldo_total'].astype(float)\n",
    "perfil_produto['media_cap84'] = perfil_produto['media_cap84'].astype(float)\n",
    "perfil_produto['media_cap85'] = perfil_produto['media_cap85'].astype(float)\n",
    "\n",
    "print(\"\\nüîß PERFIL DE PRODUTOS METAL-MEC√ÇNICO:\")\n",
    "for _, row in perfil_produto.sort_values('saldo_total', ascending=False).iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['perfil']:<40}\")\n",
    "    print(f\"    Empresas: {row['empresas']:>4,} | Risco: {row['classificacao_risco']:<10}\")\n",
    "    print(f\"    Saldo: R$ {row['saldo_total']:>15,.2f}\")\n",
    "    print(f\"    Cap 84: R$ {row['media_cap84']:>12,.2f} | Cap 85: R$ {row['media_cap85']:>12,.2f}\\n\")\n",
    "\n",
    "# Top 20 Metal-Mec√¢nico\n",
    "from pyspark.sql.functions import desc as spark_desc\n",
    "top_mm = df_metalmec.filter(\n",
    "    spark_col(\"classificacao_risco\").isin(['CR√çTICO', 'ALTO'])\n",
    ").orderBy(spark_desc(\"score_risco\")).limit(20).select(\n",
    "    \"nm_razao_social\",\n",
    "    \"saldo_credor_atual\",\n",
    "    \"vl_capitulo_84\",\n",
    "    \"vl_capitulo_85\",\n",
    "    \"perc_cap84\",\n",
    "    \"score_risco\",\n",
    "    \"classificacao_risco\"\n",
    ").toPandas()\n",
    "\n",
    "for col_name in ['saldo_credor_atual', 'vl_capitulo_84', 'vl_capitulo_85', 'perc_cap84']:\n",
    "    top_mm[col_name] = top_mm[col_name].astype(float)\n",
    "\n",
    "print(f\"üéØ TOP 20 EMPRESAS METAL-MEC√ÇNICO CR√çTICAS:\\n\")\n",
    "for idx, row in top_mm.head(20).iterrows():\n",
    "    print(f\"{idx+1:2d}. {row['nm_razao_social'][:50]:<50}\")\n",
    "    print(f\"    Saldo: R$ {row['saldo_credor_atual']:>12,.2f} | Score: {row['score_risco']:>5.0f}\")\n",
    "    print(f\"    Cap 84: R$ {row['vl_capitulo_84']:>12,.2f} ({row['perc_cap84']:>5.1f}%)\")\n",
    "    print(f\"    Cap 85: R$ {row['vl_capitulo_85']:>12,.2f}\\n\")\n",
    "\n",
    "# Visualiza√ß√£o Metal-Mec√¢nico\n",
    "fig = px.scatter(top_mm,\n",
    "                 x='vl_capitulo_84',\n",
    "                 y='vl_capitulo_85',\n",
    "                 size='saldo_credor_atual',\n",
    "                 color='classificacao_risco',\n",
    "                 hover_name='nm_razao_social',\n",
    "                 title='<b>METAL-MEC√ÇNICO: Cap 84 (M√°quinas) vs Cap 85 (El√©tricos)</b>',\n",
    "                 labels={'vl_capitulo_84': 'Valor Cap 84 (R$)',\n",
    "                        'vl_capitulo_85': 'Valor Cap 85 (R$)'},\n",
    "                 color_discrete_map={'CR√çTICO': '#d62728', 'ALTO': '#ff7f0e'})\n",
    "fig.update_layout(height=600)\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. SETOR TECNOLOGIA - AN√ÅLISE COMPLETA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3Ô∏è‚É£ SETOR TECNOLOGIA (NCM 8471xx, 8473xx, 8517xx)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_tech = spark.table(\"teste.credito_dime_tech\").filter(spark_col(\"flag_setor_tech\") == 1)\n",
    "df_tech.cache()\n",
    "\n",
    "# Estat√≠sticas gerais\n",
    "stats_tech = df_tech.agg(\n",
    "    spark_count(\"*\").alias(\"total\"),\n",
    "    spark_sum(\"vl_computadores\").alias(\"computadores_total\"),\n",
    "    spark_sum(\"vl_partes_computadores\").alias(\"partes_total\"),\n",
    "    spark_sum(\"vl_telecom\").alias(\"telecom_total\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nüìä PANORAMA SETOR TECNOLOGIA:\")\n",
    "print(f\"  ‚Ä¢ Empresas operando: {stats_tech['total']:,}\")\n",
    "print(f\"  ‚Ä¢ Computadores: R$ {stats_tech['computadores_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Partes/Perif√©ricos: R$ {stats_tech['partes_total']:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Telecom: R$ {stats_tech['telecom_total']:,.2f}\")\n",
    "\n",
    "# Perfil de produtos Tech\n",
    "perfil_tech = df_tech.withColumn(\"perfil\",\n",
    "    spark_when(spark_col(\"vl_computadores\") > (spark_col(\"vl_partes_computadores\") + spark_col(\"vl_telecom\")) * 2, \n",
    "         \"Computadores\")\n",
    "    .when(spark_col(\"vl_telecom\") > (spark_col(\"vl_computadores\") + spark_col(\"vl_partes_computadores\")) * 2,\n",
    "         \"Telecom\")\n",
    "    .when(spark_col(\"vl_partes_computadores\") > (spark_col(\"vl_computadores\") + spark_col(\"vl_telecom\")) * 2,\n",
    "         \"Partes/Perif√©ricos\")\n",
    "    .otherwise(\"Misto\")\n",
    ").groupBy(\"perfil\").agg(\n",
    "    spark_count(\"*\").alias(\"empresas\"),\n",
    "    spark_sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "    spark_sum(spark_when(spark_col(\"classificacao_risco\") == 'CR√çTICO', 1).otherwise(0)).alias(\"criticos\")\n",
    ").toPandas()\n",
    "\n",
    "perfil_tech['saldo_total'] = perfil_tech['saldo_total'].astype(float)\n",
    "\n",
    "print(\"\\nüíª PERFIL DE PRODUTOS TECNOLOGIA:\")\n",
    "for _, row in perfil_tech.iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['perfil']:<25}: {row['empresas']:>4,} empresas | Cr√≠ticos: {row['criticos']:>3,}\")\n",
    "    print(f\"    Saldo: R$ {row['saldo_total']:>15,.2f}\\n\")\n",
    "\n",
    "# Top 20 Tech\n",
    "top_tech = df_tech.filter(\n",
    "    spark_col(\"classificacao_risco\").isin(['CR√çTICO', 'ALTO'])\n",
    ").orderBy(spark_desc(\"score_risco\")).limit(20).select(\n",
    "    \"nm_razao_social\",\n",
    "    \"saldo_credor_atual\",\n",
    "    \"vl_computadores\",\n",
    "    \"vl_partes_computadores\",\n",
    "    \"vl_telecom\",\n",
    "    \"score_risco\"\n",
    ").toPandas()\n",
    "\n",
    "for col_name in ['saldo_credor_atual', 'vl_computadores', 'vl_partes_computadores', 'vl_telecom']:\n",
    "    top_tech[col_name] = top_tech[col_name].astype(float)\n",
    "\n",
    "print(f\"üéØ TOP 20 EMPRESAS TECNOLOGIA CR√çTICAS:\\n\")\n",
    "for idx, row in top_tech.head(20).iterrows():\n",
    "    total_movto = row['vl_computadores'] + row['vl_partes_computadores'] + row['vl_telecom']\n",
    "    print(f\"{idx+1:2d}. {row['nm_razao_social'][:50]:<50}\")\n",
    "    print(f\"    Saldo: R$ {row['saldo_credor_atual']:>12,.2f} | Score: {row['score_risco']:>5.0f}\")\n",
    "    print(f\"    Computadores: R$ {row['vl_computadores']:>12,.2f}\")\n",
    "    print(f\"    Partes: R$ {row['vl_partes_computadores']:>12,.2f}\")\n",
    "    print(f\"    Telecom: R$ {row['vl_telecom']:>12,.2f}\\n\")\n",
    "\n",
    "# Visualiza√ß√£o Tech - Treemap\n",
    "fig = go.Figure(go.Treemap(\n",
    "    labels=perfil_tech['perfil'],\n",
    "    parents=[\"\"] * len(perfil_tech),\n",
    "    values=perfil_tech['saldo_total'],\n",
    "    text=[f\"{e:,} empresas<br>{c:,} cr√≠ticos\" \n",
    "          for e, c in zip(perfil_tech['empresas'], perfil_tech['criticos'])],\n",
    "    textposition=\"middle center\",\n",
    "    marker=dict(colors=perfil_tech['criticos'], colorscale='Reds', showscale=True)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"<b>TECNOLOGIA: Distribui√ß√£o por Perfil de Produto</b>\",\n",
    "    height=600\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. COMPARATIVO CROSS-SETORIAL\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4Ô∏è‚É£ AN√ÅLISE CROSS-SETORIAL - EMPRESAS MULTI-SETOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Empresas que operam em m√∫ltiplos setores\n",
    "multi_setor = df_textil.alias(\"txt\").join(\n",
    "    df_metalmec.alias(\"mm\"),\n",
    "    spark_col(\"txt.nu_cnpj\") == spark_col(\"mm.nu_cnpj\"),\n",
    "    \"inner\"\n",
    ").join(\n",
    "    df_tech.alias(\"tech\"),\n",
    "    spark_col(\"txt.nu_cnpj\") == spark_col(\"tech.nu_cnpj\"),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    spark_col(\"txt.nu_cnpj\"),\n",
    "    spark_col(\"txt.nm_razao_social\"),\n",
    "    spark_col(\"txt.saldo_credor_atual\"),\n",
    "    spark_col(\"txt.vl_entradas_textil\"),\n",
    "    spark_col(\"mm.vl_entradas_metalmec\"),\n",
    "    spark_col(\"tech.vl_entradas_tech\"),\n",
    "    spark_col(\"txt.classificacao_risco\")\n",
    ").filter(\n",
    "    (spark_col(\"vl_entradas_textil\") > 0) &\n",
    "    (spark_col(\"vl_entradas_metalmec\") > 0) &\n",
    "    (spark_col(\"vl_entradas_tech\") > 0)\n",
    ").toPandas()\n",
    "\n",
    "if len(multi_setor) > 0:\n",
    "    for col_name in ['saldo_credor_atual', 'vl_entradas_textil', 'vl_entradas_metalmec', 'vl_entradas_tech']:\n",
    "        multi_setor[col_name] = multi_setor[col_name].astype(float)\n",
    "    \n",
    "    print(f\"\\nüîÑ EMPRESAS OPERANDO NOS 3 SETORES: {len(multi_setor)}\")\n",
    "    print(f\"\\nTop 10 Multi-Setor:\\n\")\n",
    "    for idx, row in multi_setor.nlargest(10, 'saldo_credor_atual').iterrows():\n",
    "        print(f\"{idx+1:2d}. {row['nm_razao_social'][:50]:<50}\")\n",
    "        print(f\"    Saldo: R$ {row['saldo_credor_atual']:>12,.2f} | Risco: {row['classificacao_risco']}\")\n",
    "        print(f\"    T√™xtil: R$ {row['vl_entradas_textil']:>12,.2f}\")\n",
    "        print(f\"    Metal-Mec: R$ {row['vl_entradas_metalmec']:>12,.2f}\")\n",
    "        print(f\"    Tech: R$ {row['vl_entradas_tech']:>12,.2f}\\n\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Nenhuma empresa opera simultaneamente nos 3 setores\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AN√ÅLISE SETORIAL DETALHADA CONCLU√çDA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Limpar cache\n",
    "df_textil.unpersist()\n",
    "df_metalmec.unpersist()\n",
    "df_tech.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b0f06-171d-43e4-9d80-7a81cc9a5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "NOTEBOOK 06: UTILITIES E EXPORTA√á√ÉO - FUN√á√ïES AUXILIARES\n",
    "=============================================================================\n",
    "Fun√ß√µes √∫teis para consultas ad-hoc e exporta√ß√£o de resultados\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col as spark_col, when as spark_when, desc as spark_desc,\n",
    "    sum as spark_sum, avg as spark_avg, count as spark_count,\n",
    "    max as spark_max, min as spark_min, countDistinct\n",
    ")\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üõ†Ô∏è UTILITIES E FUN√á√ïES AUXILIARES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. FUN√á√ïES DE CONSULTA R√ÅPIDA\n",
    "# =============================================================================\n",
    "\n",
    "def buscar_empresa(cnpj=None, razao_social=None):\n",
    "    \"\"\"\n",
    "    Busca informa√ß√µes completas de uma empresa\n",
    "    \n",
    "    Args:\n",
    "        cnpj: CNPJ da empresa (com ou sem formata√ß√£o)\n",
    "        razao_social: Nome da empresa (busca parcial)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame pandas com informa√ß√µes da empresa\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.table(\"dados_finais_com_predicao\")\n",
    "    except:\n",
    "        df = spark.table(\"teste.credito_dime_completo\")\n",
    "    \n",
    "    if cnpj:\n",
    "        cnpj_limpo = ''.join(filter(str.isdigit, str(cnpj)))\n",
    "        resultado = df.filter(spark_col(\"nu_cnpj\") == cnpj_limpo)\n",
    "    elif razao_social:\n",
    "        resultado = df.filter(spark_col(\"nm_razao_social\").like(f\"%{razao_social}%\"))\n",
    "    else:\n",
    "        print(\"‚ùå Forne√ßa CNPJ ou Raz√£o Social\")\n",
    "        return None\n",
    "    \n",
    "    if resultado.count() == 0:\n",
    "        print(\"‚ùå Empresa n√£o encontrada\")\n",
    "        return None\n",
    "    \n",
    "    df_resultado = resultado.select(\n",
    "        \"nu_cnpj\",\n",
    "        \"nm_razao_social\",\n",
    "        \"nm_fantasia\",\n",
    "        \"nm_gerfe\",\n",
    "        \"de_cnae\",\n",
    "        \"score_risco\",\n",
    "        \"classificacao_risco\",\n",
    "        \"saldo_credor_atual\",\n",
    "        \"crescimento_saldo_percentual\",\n",
    "        \"qtde_ultimos_meses_iguais\",\n",
    "        \"vl_credito_presumido_13m\"\n",
    "    ).toPandas()\n",
    "    \n",
    "    # Converter Decimal para float\n",
    "    for col_name in df_resultado.columns:\n",
    "        if df_resultado[col_name].dtype == 'object':\n",
    "            try:\n",
    "                df_resultado[col_name] = df_resultado[col_name].astype(float)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return df_resultado\n",
    "\n",
    "\n",
    "def listar_por_gerfe(gerfe, risco='CR√çTICO'):\n",
    "    \"\"\"\n",
    "    Lista empresas de uma GERFE espec√≠fica por n√≠vel de risco\n",
    "    \n",
    "    Args:\n",
    "        gerfe: Nome da GERFE\n",
    "        risco: Classifica√ß√£o de risco (CR√çTICO, ALTO, M√âDIO, BAIXO)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame pandas ordenado por score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.table(\"dados_finais_com_predicao\")\n",
    "    except:\n",
    "        df = spark.table(\"teste.credito_dime_completo\")\n",
    "    \n",
    "    resultado = df.filter(\n",
    "        (spark_col(\"nm_gerfe\").like(f\"%{gerfe}%\")) &\n",
    "        (spark_col(\"classificacao_risco\") == risco)\n",
    "    ).orderBy(spark_desc(\"score_risco\")).select(\n",
    "        \"nu_cnpj\",\n",
    "        \"nm_razao_social\",\n",
    "        \"score_risco\",\n",
    "        \"saldo_credor_atual\",\n",
    "        \"qtde_ultimos_meses_iguais\",\n",
    "        \"crescimento_saldo_percentual\"\n",
    "    ).toPandas()\n",
    "    \n",
    "    for col_name in ['saldo_credor_atual', 'crescimento_saldo_percentual']:\n",
    "        resultado[col_name] = resultado[col_name].astype(float)\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "\n",
    "def ranking_por_criterio(criterio='score_risco', limite=50, risco_minimo='ALTO'):\n",
    "    \"\"\"\n",
    "    Gera ranking personalizado\n",
    "    \n",
    "    Args:\n",
    "        criterio: Coluna para ordena√ß√£o (score_risco, saldo_credor_atual, crescimento_saldo_percentual)\n",
    "        limite: N√∫mero de registros\n",
    "        risco_minimo: Filtro m√≠nimo de risco\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame pandas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.table(\"dados_finais_com_predicao\")\n",
    "    except:\n",
    "        df = spark.table(\"teste.credito_dime_completo\")\n",
    "    \n",
    "    filtros_risco = {\n",
    "        'CR√çTICO': ['CR√çTICO'],\n",
    "        'ALTO': ['CR√çTICO', 'ALTO'],\n",
    "        'M√âDIO': ['CR√çTICO', 'ALTO', 'M√âDIO']\n",
    "    }\n",
    "    \n",
    "    resultado = df.filter(\n",
    "        spark_col(\"classificacao_risco\").isin(filtros_risco.get(risco_minimo, ['CR√çTICO', 'ALTO']))\n",
    "    ).orderBy(spark_desc(criterio)).limit(limite).select(\n",
    "        \"nu_cnpj\",\n",
    "        \"nm_razao_social\",\n",
    "        \"nm_gerfe\",\n",
    "        \"score_risco\",\n",
    "        \"classificacao_risco\",\n",
    "        \"saldo_credor_atual\",\n",
    "        \"qtde_ultimos_meses_iguais\",\n",
    "        \"crescimento_saldo_percentual\"\n",
    "    ).toPandas()\n",
    "    \n",
    "    for col_name in ['saldo_credor_atual', 'crescimento_saldo_percentual']:\n",
    "        resultado[col_name] = resultado[col_name].astype(float)\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "\n",
    "def estatisticas_gerais():\n",
    "    \"\"\"Retorna estat√≠sticas gerais do sistema\"\"\"\n",
    "    try:\n",
    "        df = spark.table(\"dados_finais_com_predicao\")\n",
    "    except:\n",
    "        df = spark.table(\"teste.credito_dime_completo\")\n",
    "    \n",
    "    stats = df.agg(\n",
    "        spark_count(\"*\").alias(\"total_empresas\"),\n",
    "        countDistinct(\"nu_cnpj_grupo\").alias(\"total_grupos\"),\n",
    "        spark_sum(\"saldo_credor_atual\").alias(\"saldo_total\"),\n",
    "        spark_avg(\"score_risco\").alias(\"score_medio\"),\n",
    "        spark_sum(spark_when(spark_col(\"classificacao_risco\") == 'CR√çTICO', 1).otherwise(0)).alias(\"criticos\"),\n",
    "        spark_sum(spark_when(spark_col(\"classificacao_risco\") == 'ALTO', 1).otherwise(0)).alias(\"altos\"),\n",
    "        spark_sum(spark_when(spark_col(\"qtde_ultimos_meses_iguais\") >= 12, 1).otherwise(0)).alias(\"congelados_12m\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    resultado = {\n",
    "        'Total de Empresas': f\"{stats['total_empresas']:,}\",\n",
    "        'Total de Grupos Econ√¥micos': f\"{stats['total_grupos']:,}\",\n",
    "        'Saldo Credor Total': f\"R$ {stats['saldo_total']:,.2f}\",\n",
    "        'Score M√©dio': f\"{stats['score_medio']:.1f}\",\n",
    "        'Casos Cr√≠ticos': f\"{stats['criticos']:,}\",\n",
    "        'Casos Alto Risco': f\"{stats['altos']:,}\",\n",
    "        'Empresas Congeladas ‚â•12m': f\"{stats['congelados_12m']:,}\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä ESTAT√çSTICAS GERAIS DO SISTEMA\\n\")\n",
    "    for chave, valor in resultado.items():\n",
    "        print(f\"  ‚Ä¢ {chave:<35}: {valor:>15}\")\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "\n",
    "def comparar_periodos(periodo1='202409', periodo2='202509'):\n",
    "    \"\"\"\n",
    "    Compara m√©tricas entre dois per√≠odos\n",
    "    \n",
    "    Args:\n",
    "        periodo1: Per√≠odo inicial (formato YYYYMM)\n",
    "        periodo2: Per√≠odo final (formato YYYYMM)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com compara√ß√£o\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä COMPARA√á√ÉO DE PER√çODOS: {periodo1} ‚Üí {periodo2}\\n\")\n",
    "    print(\"‚ö†Ô∏è Nota: Esta fun√ß√£o usa dados agregados atuais como proxy\")\n",
    "    print(\"    Para an√°lise temporal real, √© necess√°rio hist√≥rico de per√≠odos\\n\")\n",
    "    \n",
    "    try:\n",
    "        df = spark.table(\"dados_finais_com_predicao\")\n",
    "    except:\n",
    "        df = spark.table(\"teste.credito_dime_completo\")\n",
    "    \n",
    "    # Simular compara√ß√£o usando dados de crescimento\n",
    "    stats = df.agg(\n",
    "        spark_count(\"*\").alias(\"total\"),\n",
    "        spark_avg(\"saldo_13m_atras\").alias(\"saldo_anterior\"),\n",
    "        spark_avg(\"saldo_credor_atual\").alias(\"saldo_atual\"),\n",
    "        spark_avg(\"crescimento_saldo_percentual\").alias(\"crescimento_medio\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    comparacao = {\n",
    "        'M√©trica': ['Total Empresas', 'Saldo M√©dio', 'Crescimento M√©dio'],\n",
    "        'Per√≠odo Anterior': [\n",
    "            f\"{stats['total']:,}\",\n",
    "            f\"R$ {stats['saldo_anterior']:,.2f}\",\n",
    "            f\"Base\"\n",
    "        ],\n",
    "        'Per√≠odo Atual': [\n",
    "            f\"{stats['total']:,}\",\n",
    "            f\"R$ {stats['saldo_atual']:,.2f}\",\n",
    "            f\"{stats['crescimento_medio']:+.1f}%\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_comp = pd.DataFrame(comparacao)\n",
    "    print(df_comp.to_string(index=False))\n",
    "    \n",
    "    return df_comp\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. FUN√á√ïES DE EXPORTA√á√ÉO\n",
    "# =============================================================================\n",
    "\n",
    "def exportar_lista_fiscalizacao(nivel_risco='CR√çTICO', formato='csv', caminho='./'):\n",
    "    \"\"\"\n",
    "    Exporta lista de empresas para fiscaliza√ß√£o\n",
    "    \n",
    "    Args:\n",
    "        nivel_risco: CR√çTICO, ALTO, M√âDIO\n",
    "        formato: csv, excel, parquet\n",
    "        caminho: Diret√≥rio de sa√≠da\n",
    "    \n",
    "    Returns:\n",
    "        String com caminho do arquivo gerado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.table(\"dados_finais_com_predicao\")\n",
    "    except:\n",
    "        df = spark.table(\"teste.credito_dime_completo\")\n",
    "    \n",
    "    filtros_risco = {\n",
    "        'CR√çTICO': ['CR√çTICO'],\n",
    "        'ALTO': ['CR√çTICO', 'ALTO'],\n",
    "        'M√âDIO': ['CR√çTICO', 'ALTO', 'M√âDIO']\n",
    "    }\n",
    "    \n",
    "    lista = df.filter(\n",
    "        spark_col(\"classificacao_risco\").isin(filtros_risco[nivel_risco])\n",
    "    ).orderBy(spark_desc(\"score_risco\")).select(\n",
    "        \"nu_cnpj\",\n",
    "        \"nm_razao_social\",\n",
    "        \"nm_fantasia\",\n",
    "        \"nm_gerfe\",\n",
    "        \"de_cnae\",\n",
    "        \"score_risco\",\n",
    "        \"classificacao_risco\",\n",
    "        \"saldo_credor_atual\",\n",
    "        \"saldo_13m_atras\",\n",
    "        \"crescimento_saldo_percentual\",\n",
    "        \"qtde_ultimos_meses_iguais\",\n",
    "        \"valor_sequencia_recente\",\n",
    "        \"vl_credito_presumido_13m\"\n",
    "    ).toPandas()\n",
    "    \n",
    "    # Converter Decimal\n",
    "    for col_name in lista.columns:\n",
    "        if lista[col_name].dtype == 'object':\n",
    "            try:\n",
    "                lista[col_name] = lista[col_name].astype(float)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    arquivo_base = f\"lista_fiscalizacao_{nivel_risco}_{timestamp}\"\n",
    "    \n",
    "    if formato == 'csv':\n",
    "        arquivo = os.path.join(caminho, f\"{arquivo_base}.csv\")\n",
    "        lista.to_csv(arquivo, index=False, encoding='utf-8-sig', sep=';')\n",
    "    elif formato == 'excel':\n",
    "        arquivo = os.path.join(caminho, f\"{arquivo_base}.xlsx\")\n",
    "        lista.to_excel(arquivo, index=False, engine='openpyxl')\n",
    "    elif formato == 'parquet':\n",
    "        arquivo = os.path.join(caminho, f\"{arquivo_base}.parquet\")\n",
    "        lista.to_parquet(arquivo, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Arquivo exportado: {arquivo}\")\n",
    "    print(f\"   Total de registros: {len(lista):,}\")\n",
    "    \n",
    "    return arquivo\n",
    "\n",
    "\n",
    "def gerar_relatorio_consolidado(caminho='./'):\n",
    "    \"\"\"\n",
    "    Gera relat√≥rio consolidado completo em Excel com m√∫ltiplas abas\n",
    "    \n",
    "    Args:\n",
    "        caminho: Diret√≥rio de sa√≠da\n",
    "    \n",
    "    Returns:\n",
    "        Caminho do arquivo gerado\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüìÑ GERANDO RELAT√ìRIO CONSOLIDADO COMPLETO...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        df = spark.table(\"dados_finais_com_predicao\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Usando tabela principal (view ML n√£o encontrada)\")\n",
    "        df = spark.table(\"teste.credito_dime_completo\")\n",
    "    \n",
    "    # 1. Resumo Executivo\n",
    "    print(\"  1/7 Preparando Resumo Executivo...\")\n",
    "    stats = df.agg(\n",
    "        spark_count(\"*\").alias(\"Total Empresas\"),\n",
    "        spark_sum(\"saldo_credor_atual\").alias(\"Saldo Total\"),\n",
    "        spark_avg(\"score_risco\").alias(\"Score M√©dio\"),\n",
    "        spark_sum(spark_when(spark_col(\"classificacao_risco\") == 'CR√çTICO', 1).otherwise(0)).alias(\"Casos Cr√≠ticos\"),\n",
    "        spark_sum(spark_when(spark_col(\"classificacao_risco\") == 'ALTO', 1).otherwise(0)).alias(\"Casos Alto Risco\")\n",
    "    ).toPandas().T\n",
    "    stats.columns = ['Valor']\n",
    "    stats['Valor'] = stats['Valor'].astype(float)\n",
    "    \n",
    "    # 2. Top 100 Priorit√°rias\n",
    "    print(\"  2/7 Preparando Top 100 Empresas...\")\n",
    "    top_100 = ranking_por_criterio(criterio='score_risco', limite=100, risco_minimo='ALTO')\n",
    "    \n",
    "    # 3. Distribui√ß√£o por Risco\n",
    "    print(\"  3/7 Preparando Distribui√ß√£o por Risco...\")\n",
    "    dist_risco = df.groupBy(\"classificacao_risco\").agg(\n",
    "        spark_count(\"*\").alias(\"Quantidade\"),\n",
    "        spark_sum(\"saldo_credor_atual\").alias(\"Saldo Total\")\n",
    "    ).toPandas()\n",
    "    dist_risco['Saldo Total'] = dist_risco['Saldo Total'].astype(float)\n",
    "    \n",
    "    # 4. An√°lise por GERFE\n",
    "    print(\"  4/7 Preparando An√°lise por GERFE...\")\n",
    "    por_gerfe = df.groupBy(\"nm_gerfe\").agg(\n",
    "        spark_count(\"*\").alias(\"Total\"),\n",
    "        spark_sum(spark_when(spark_col(\"classificacao_risco\").isin(['CR√çTICO','ALTO']), 1).otherwise(0)).alias(\"Priorit√°rios\"),\n",
    "        spark_sum(\"saldo_credor_atual\").alias(\"Saldo Total\")\n",
    "    ).orderBy(spark_desc(\"Priorit√°rios\")).toPandas()\n",
    "    por_gerfe['Saldo Total'] = por_gerfe['Saldo Total'].astype(float)\n",
    "    \n",
    "    # 5. Empresas Congeladas\n",
    "    print(\"  5/7 Preparando Empresas Congeladas...\")\n",
    "    congeladas = df.filter(\n",
    "        spark_col(\"qtde_ultimos_meses_iguais\") >= 12\n",
    "    ).orderBy(spark_desc(\"saldo_credor_atual\")).limit(100).select(\n",
    "        \"nu_cnpj\",\n",
    "        \"nm_razao_social\",\n",
    "        \"nm_gerfe\",\n",
    "        \"saldo_credor_atual\",\n",
    "        \"qtde_ultimos_meses_iguais\",\n",
    "        \"valor_sequencia_recente\",\n",
    "        \"classificacao_risco\"\n",
    "    ).toPandas()\n",
    "    congeladas['saldo_credor_atual'] = congeladas['saldo_credor_atual'].astype(float)\n",
    "    congeladas['valor_sequencia_recente'] = congeladas['valor_sequencia_recente'].astype(float)\n",
    "    \n",
    "    # 6. Crescimento Explosivo\n",
    "    print(\"  6/7 Preparando Casos de Crescimento Explosivo...\")\n",
    "    crescimento = df.filter(\n",
    "        spark_col(\"crescimento_saldo_percentual\") > 200\n",
    "    ).orderBy(spark_desc(\"crescimento_saldo_percentual\")).limit(100).select(\n",
    "        \"nu_cnpj\",\n",
    "        \"nm_razao_social\",\n",
    "        \"nm_gerfe\",\n",
    "        \"saldo_13m_atras\",\n",
    "        \"saldo_credor_atual\",\n",
    "        \"crescimento_saldo_percentual\",\n",
    "        \"classificacao_risco\"\n",
    "    ).toPandas()\n",
    "    for col_name in ['saldo_13m_atras', 'saldo_credor_atual', 'crescimento_saldo_percentual']:\n",
    "        crescimento[col_name] = crescimento[col_name].astype(float)\n",
    "    \n",
    "    # Exportar para Excel\n",
    "    print(\"  7/7 Salvando arquivo Excel...\")\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    arquivo = os.path.join(caminho, f\"relatorio_consolidado_{timestamp}.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(arquivo, engine='openpyxl') as writer:\n",
    "        stats.to_excel(writer, sheet_name='1-Resumo Executivo')\n",
    "        top_100.to_excel(writer, sheet_name='2-Top 100 Priorit√°rias', index=False)\n",
    "        dist_risco.to_excel(writer, sheet_name='3-Distribui√ß√£o Risco', index=False)\n",
    "        por_gerfe.to_excel(writer, sheet_name='4-An√°lise GERFE', index=False)\n",
    "        congeladas.to_excel(writer, sheet_name='5-Congeladas 12m+', index=False)\n",
    "        crescimento.to_excel(writer, sheet_name='6-Crescimento Explosivo', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ RELAT√ìRIO CONSOLIDADO GERADO COM SUCESSO!\")\n",
    "    print(f\"   Arquivo: {arquivo}\")\n",
    "    print(f\"   Abas: 6\")\n",
    "    print(f\"   Total de linhas: {len(top_100) + len(dist_risco) + len(por_gerfe) + len(congeladas) + len(crescimento):,}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return arquivo\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ UTILITIES CARREGADAS COM SUCESSO!\")\n",
    "print(\"\\nüí° Fun√ß√µes dispon√≠veis:\")\n",
    "print(\"   ‚Ä¢ buscar_empresa(cnpj, razao_social)\")\n",
    "print(\"   ‚Ä¢ listar_por_gerfe(gerfe, risco)\")\n",
    "print(\"   ‚Ä¢ ranking_por_criterio(criterio, limite, risco_minimo)\")\n",
    "print(\"   ‚Ä¢ estatisticas_gerais()\")\n",
    "print(\"   ‚Ä¢ comparar_periodos(periodo1, periodo2)\")\n",
    "print(\"   ‚Ä¢ exportar_lista_fiscalizacao(nivel_risco, formato, caminho)\")\n",
    "print(\"   ‚Ä¢ gerar_relatorio_consolidado(caminho)\")\n",
    "print(\"\\nüöÄ Sistema pronto para uso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e77c37-0542-4e55-983c-806aec3aac1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "VALIDA√á√ÉO FINAL DO SISTEMA - CHECKLIST COMPLETO\n",
    "=============================================================================\n",
    "Execute este script para validar que TUDO est√° funcionando\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç VALIDA√á√ÉO FINAL DO SISTEMA - CHECKLIST COMPLETO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# 1. VALIDAR IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Validando imports...\")\n",
    "\n",
    "try:\n",
    "    from pyspark.sql.functions import (\n",
    "        col as spark_col, when as spark_when, desc as spark_desc,\n",
    "        sum as spark_sum, avg as spark_avg, count as spark_count\n",
    "    )\n",
    "    print(\"  ‚úÖ PySpark imports OK\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erro nos imports PySpark: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"  ‚úÖ Pandas e NumPy OK\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erro nos imports Python: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    print(\"  ‚úÖ Plotly OK\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erro no Plotly: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. VALIDAR SESS√ÉO SPARK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ Validando sess√£o Spark...\")\n",
    "\n",
    "try:\n",
    "    app_id = spark.sparkContext.applicationId\n",
    "    print(f\"  ‚úÖ Sess√£o Spark ativa: {app_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Sess√£o Spark n√£o encontrada: {e}\")\n",
    "    print(\"     Execute primeiro o notebook de setup\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. VALIDAR TABELAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Validando tabelas...\")\n",
    "\n",
    "tabelas_obrigatorias = [\n",
    "    'teste.credito_dime_completo',\n",
    "    'teste.credito_dime_textil',\n",
    "    'teste.credito_dime_metalmec',\n",
    "    'teste.credito_dime_tech'\n",
    "]\n",
    "\n",
    "todas_ok = True\n",
    "for tabela in tabelas_obrigatorias:\n",
    "    try:\n",
    "        count = spark.table(tabela).count()\n",
    "        print(f\"  ‚úÖ {tabela}: {count:,} registros\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {tabela}: N√£o encontrada\")\n",
    "        todas_ok = False\n",
    "\n",
    "if not todas_ok:\n",
    "    print(\"\\n  ‚ö†Ô∏è Execute os scripts SQL de cria√ß√£o das tabelas primeiro\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. VALIDAR VIEW ML\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Validando view de Machine Learning...\")\n",
    "\n",
    "try:\n",
    "    count = spark.table(\"dados_finais_com_predicao\").count()\n",
    "    print(f\"  ‚úÖ View 'dados_finais_com_predicao': {count:,} registros\")\n",
    "    \n",
    "    # Verificar colunas essenciais\n",
    "    df_test = spark.table(\"dados_finais_com_predicao\")\n",
    "    colunas_ml = ['prob_risco_ml', 'nivel_alerta_ml', 'prediction']\n",
    "    \n",
    "    for col_name in colunas_ml:\n",
    "        if col_name in df_test.columns:\n",
    "            print(f\"     ‚úÖ Coluna '{col_name}' presente\")\n",
    "        else:\n",
    "            print(f\"     ‚ö†Ô∏è Coluna '{col_name}' ausente\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è View n√£o encontrada (Execute Notebook 03 para criar)\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. TESTAR CONVERS√ïES DECIMAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Testando convers√µes Decimal ‚Üí float...\")\n",
    "\n",
    "try:\n",
    "    df_test = spark.table(\"teste.credito_dime_completo\").limit(5).toPandas()\n",
    "    \n",
    "    # Tentar converter colunas num√©ricas\n",
    "    conversoes_ok = 0\n",
    "    for col_name in ['saldo_credor_atual', 'score_risco', 'crescimento_saldo_percentual']:\n",
    "        if col_name in df_test.columns:\n",
    "            try:\n",
    "                df_test[col_name] = df_test[col_name].astype(float)\n",
    "                conversoes_ok += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(f\"  ‚úÖ {conversoes_ok} colunas convertidas com sucesso\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Erro no teste de convers√£o: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. TESTAR FUN√á√ïES SPARK COM ALIASES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n6Ô∏è‚É£ Testando fun√ß√µes Spark com aliases...\")\n",
    "\n",
    "try:\n",
    "    from pyspark.sql.functions import (\n",
    "        col as spark_col,\n",
    "        sum as spark_sum,\n",
    "        avg as spark_avg,\n",
    "        desc as spark_desc\n",
    "    )\n",
    "    \n",
    "    # Teste simples\n",
    "    df = spark.table(\"teste.credito_dime_completo\").limit(10)\n",
    "    resultado = df.filter(spark_col(\"score_risco\") > 0).count()\n",
    "    \n",
    "    print(f\"  ‚úÖ Aliases funcionando (teste: {resultado} registros)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erro com aliases: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. TESTAR VISUALIZA√á√ïES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n7Ô∏è‚É£ Testando capacidade de visualiza√ß√£o...\")\n",
    "\n",
    "try:\n",
    "    # Criar um gr√°fico simples\n",
    "    fig = go.Figure(data=go.Bar(x=['A', 'B'], y=[1, 2]))\n",
    "    print(\"  ‚úÖ Plotly funcionando\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è Erro no Plotly: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. VERIFICAR CONFLITOS DE NAMESPACE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n8Ô∏è‚É£ Verificando conflitos no namespace...\")\n",
    "\n",
    "import inspect\n",
    "frame = inspect.currentframe()\n",
    "namespace = frame.f_globals\n",
    "\n",
    "conflitos_encontrados = []\n",
    "funcoes_criticas = ['col', 'sum', 'max', 'min']\n",
    "\n",
    "for func in funcoes_criticas:\n",
    "    if func in namespace:\n",
    "        obj = namespace[func]\n",
    "        if isinstance(obj, str):\n",
    "            conflitos_encontrados.append(func)\n",
    "\n",
    "if conflitos_encontrados:\n",
    "    print(f\"  ‚ö†Ô∏è Conflitos detectados: {', '.join(conflitos_encontrados)}\")\n",
    "    print(\"     Solu√ß√£o: Restart kernel e use aliases (spark_col, spark_sum, etc)\")\n",
    "else:\n",
    "    print(\"  ‚úÖ Nenhum conflito detectado\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9. RESUMO FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMO DA VALIDA√á√ÉO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "status_items = []\n",
    "\n",
    "# Imports\n",
    "status_items.append((\"Imports Python/Spark\", True))\n",
    "\n",
    "# Sess√£o\n",
    "try:\n",
    "    spark.sparkContext.applicationId\n",
    "    status_items.append((\"Sess√£o Spark\", True))\n",
    "except:\n",
    "    status_items.append((\"Sess√£o Spark\", False))\n",
    "\n",
    "# Tabelas\n",
    "status_items.append((\"Tabelas principais\", todas_ok))\n",
    "\n",
    "# View ML\n",
    "try:\n",
    "    spark.table(\"dados_finais_com_predicao\").count()\n",
    "    status_items.append((\"View ML\", True))\n",
    "except:\n",
    "    status_items.append((\"View ML\", False))\n",
    "\n",
    "# Visualiza√ß√µes\n",
    "status_items.append((\"Plotly\", True))\n",
    "\n",
    "# Conflitos\n",
    "status_items.append((\"Sem conflitos\", len(conflitos_encontrados) == 0))\n",
    "\n",
    "# Calcular score - CORRE√á√ÉO AQUI: usar sum() nativo do Python\n",
    "total_checks = len(status_items)\n",
    "passed_checks = 0\n",
    "for _, status in status_items:\n",
    "    if status:\n",
    "        passed_checks += 1\n",
    "\n",
    "score = (passed_checks / total_checks) * 100\n",
    "\n",
    "print(\"\\nüìã Checklist:\")\n",
    "for item, status in status_items:\n",
    "    emoji = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"  {emoji} {item}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Score: {passed_checks}/{total_checks} ({score:.0f}%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if score == 100:\n",
    "    print(\"\\nüéâ SISTEMA 100% OPERACIONAL!\")\n",
    "    print(\"\\n‚ú® Pr√≥ximos passos:\")\n",
    "    print(\"   1. Use as fun√ß√µes auxiliares:\")\n",
    "    print(\"      estatisticas_gerais()\")\n",
    "    print(\"      buscar_empresa(cnpj='...')\")\n",
    "    print(\"      gerar_relatorio_consolidado()\")\n",
    "    print(\"   2. Execute os notebooks de an√°lise na ordem\")\n",
    "    \n",
    "elif score >= 80:\n",
    "    print(\"\\n‚ö†Ô∏è Sistema quase pronto (alguns itens opcionais pendentes)\")\n",
    "    print(\"\\nüí° Itens pendentes:\")\n",
    "    for item, status in status_items:\n",
    "        if not status:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå Sistema requer aten√ß√£o!\")\n",
    "    print(\"\\nüîß A√ß√µes necess√°rias:\")\n",
    "    for item, status in status_items:\n",
    "        if not status:\n",
    "            if item == \"Sess√£o Spark\":\n",
    "                print(f\"   ‚Ä¢ {item}: Execute o notebook de setup\")\n",
    "            elif item == \"Tabelas principais\":\n",
    "                print(f\"   ‚Ä¢ {item}: Execute os scripts SQL de cria√ß√£o\")\n",
    "            elif item == \"View ML\":\n",
    "                print(f\"   ‚Ä¢ {item}: Execute o notebook de Machine Learning\")\n",
    "            elif \"conflitos\" in item.lower():\n",
    "                print(f\"   ‚Ä¢ {item}: Restart kernel e use aliases\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {item}: Verifique instala√ß√£o\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Valida√ß√£o conclu√≠da em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430a004-4af9-4c93-9bbc-59a8014c1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "SCRIPT PARA SALVAR TODOS OS NOTEBOOKS NO DIRET√ìRIO CORRETO\n",
    "===============================================================================\n",
    "Execute este script no Jupyter Notebook para salvar todos os arquivos\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configura√ß√µes\n",
    "INSTALL_DIR = \"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc/tsevero\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üíæ SALVANDO NOTEBOOKS NO DIRET√ìRIO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÇ Diret√≥rio: {INSTALL_DIR}\\n\")\n",
    "\n",
    "# Verificar se diret√≥rio existe\n",
    "if not os.path.exists(INSTALL_DIR):\n",
    "    print(f\"‚ùå ERRO: Diret√≥rio n√£o existe!\")\n",
    "    print(f\"   Criando diret√≥rio: {INSTALL_DIR}\")\n",
    "    try:\n",
    "        os.makedirs(INSTALL_DIR, exist_ok=True)\n",
    "        print(\"‚úÖ Diret√≥rio criado com sucesso\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao criar diret√≥rio: {e}\")\n",
    "        exit(1)\n",
    "else:\n",
    "    print(\"‚úÖ Diret√≥rio encontrado\")\n",
    "\n",
    "# Criar backup se necess√°rio\n",
    "backup_dir = os.path.join(INSTALL_DIR, f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "\n",
    "print(f\"\\nüíæ Backup ser√° salvo em: {backup_dir}\")\n",
    "\n",
    "# Lista de arquivos (nome do arquivo, conte√∫do em string)\n",
    "# NOTA: Na pr√°tica, voc√™ copiaria o conte√∫do dos artifacts aqui\n",
    "# Por simplicidade, vou criar um exemplo de estrutura\n",
    "\n",
    "arquivos = {\n",
    "    \"SOLUCAO_DEFINITIVA_CONFLITOS.py\": \"\"\"# Conte√∫do do SOLUCAO_DEFINITIVA_CONFLITOS.py\n",
    "# Copie o conte√∫do do artifact aqui\n",
    "\"\"\",\n",
    "    \"00_correcoes_e_dicas.py\": \"# Notebook 00\",\n",
    "    \"01_setup_conexao.py\": \"# Notebook 01\",\n",
    "    \"02_analise_exploratoria_eda.py\": \"# Notebook 02\",\n",
    "    \"03_machine_learning_pipeline.py\": \"# Notebook 03\",\n",
    "    \"04_dashboards_interativos.py\": \"# Notebook 04\",\n",
    "    \"05_analise_setorial_detalhada.py\": \"# Notebook 05\",\n",
    "    \"06_utilities_exportacao.py\": \"# Notebook 06\",\n",
    "    \"VALIDACAO_FINAL_SISTEMA.py\": \"# Valida√ß√£o\",\n",
    "    \"README_Sistema_Analise_Creditos.md\": \"# README\",\n",
    "    \"GUIA_EXECUCAO_FINAL.md\": \"# Guia de Execu√ß√£o\",\n",
    "    \"GUIA_CORRECOES_RAPIDAS.md\": \"# Guia de Corre√ß√µes\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì• INSTRU√á√ïES PARA SALVAR OS ARQUIVOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Como os notebooks foram criados na conversa, voc√™ precisa:\n",
    "\n",
    "1Ô∏è‚É£ COPIAR MANUALMENTE cada notebook dos artifacts acima\n",
    "\n",
    "2Ô∏è‚É£ OU usar este comando para cada notebook:\n",
    "\n",
    "   No seu terminal/Jupyter, execute:\n",
    "\n",
    "   cd /home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc/tsevero\n",
    "\n",
    "3Ô∏è‚É£ Criar cada arquivo .py com o conte√∫do dos artifacts:\n",
    "\n",
    "   Exemplo para SOLUCAO_DEFINITIVA_CONFLITOS.py:\n",
    "   \n",
    "   %%writefile SOLUCAO_DEFINITIVA_CONFLITOS.py\n",
    "   [Cole aqui o conte√∫do do artifact]\n",
    "\n",
    "4Ô∏è‚É£ OU baixar todos de uma vez (se dispon√≠vel via Claude):\n",
    "   \n",
    "   - Clique em cada artifact\n",
    "   - Clique em \"Download\" ou copie o conte√∫do\n",
    "   - Salve no diret√≥rio correto\n",
    "\n",
    "===============================================================================\n",
    "üìã LISTA DE ARQUIVOS PARA SALVAR (em ordem de prioridade):\n",
    "===============================================================================\n",
    "\"\"\")\n",
    "\n",
    "prioridade = {\n",
    "    1: [\"SOLUCAO_DEFINITIVA_CONFLITOS.py\", \"üõ°Ô∏è CR√çTICO - Execute sempre primeiro\"],\n",
    "    2: [\"VALIDACAO_FINAL_SISTEMA.py\", \"üîç Valida√ß√£o do sistema\"],\n",
    "    3: [\"00_correcoes_e_dicas.py\", \"üîß Troubleshooting\"],\n",
    "    4: [\"01_setup_conexao.py\", \"‚öôÔ∏è Setup\"],\n",
    "    5: [\"02_analise_exploratoria_eda.py\", \"üìä EDA\"],\n",
    "    6: [\"03_machine_learning_pipeline.py\", \"ü§ñ Machine Learning\"],\n",
    "    7: [\"04_dashboards_interativos.py\", \"üìà Dashboards\"],\n",
    "    8: [\"05_analise_setorial_detalhada.py\", \"üè≠ An√°lise Setorial\"],\n",
    "    9: [\"06_utilities_exportacao.py\", \"üõ†Ô∏è Utilities\"],\n",
    "    10: [\"README_Sistema_Analise_Creditos.md\", \"üìñ Documenta√ß√£o\"],\n",
    "    11: [\"GUIA_EXECUCAO_FINAL.md\", \"üéØ Guia de Execu√ß√£o\"],\n",
    "    12: [\"GUIA_CORRECOES_RAPIDAS.md\", \"üÜò Troubleshooting Guia\"]\n",
    "}\n",
    "\n",
    "for ordem in sorted(prioridade.keys()):\n",
    "    arquivo, descricao = prioridade[ordem]\n",
    "    print(f\"  {ordem:2d}. {descricao:<40} ‚Üí {arquivo}\")\n",
    "\n",
    "print(\"\"\"\n",
    "===============================================================================\n",
    "üí° M√âTODO R√ÅPIDO - Usando %%writefile no Jupyter:\n",
    "===============================================================================\n",
    "\n",
    "Para cada arquivo, crie uma c√©lula nova e execute:\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "# Exemplo para arquivo 1:\n",
    "%%writefile /home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc/tsevero/SOLUCAO_DEFINITIVA_CONFLITOS.py\n",
    "[Cole todo o conte√∫do do artifact SOLUCAO_DEFINITIVA_CONFLITOS.py aqui]\n",
    "\n",
    "# Exemplo para arquivo 2:\n",
    "%%writefile /home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc/tsevero/01_setup_conexao.py\n",
    "[Cole todo o conte√∫do do artifact 01_setup_conexao.py aqui]\n",
    "\n",
    "... e assim por diante para todos os 12 arquivos\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "===============================================================================\n",
    "‚úÖ VERIFICAR AP√ìS SALVAR:\n",
    "===============================================================================\n",
    "\n",
    "Execute no Jupyter:\n",
    "\n",
    "import os\n",
    "dir_path = \"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc/tsevero\"\n",
    "arquivos = [f for f in os.listdir(dir_path) if f.endswith('.py') or f.endswith('.md')]\n",
    "print(f\"Total de arquivos salvos: {len(arquivos)}\")\n",
    "for arquivo in sorted(arquivos):\n",
    "    tamanho = os.path.getsize(os.path.join(dir_path, arquivo))\n",
    "    print(f\"  ‚úÖ {arquivo:<50} ({tamanho:,} bytes)\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ AP√ìS SALVAR TODOS OS ARQUIVOS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "cd /home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc/tsevero\n",
    "\n",
    "# Validar instala√ß√£o:\n",
    "%run VALIDACAO_FINAL_SISTEMA.py\n",
    "\n",
    "# Se tudo OK (Score 100%), come√ßar a usar:\n",
    "%run SOLUCAO_DEFINITIVA_CONFLITOS.py\n",
    "%run 02_analise_exploratoria_eda.py\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úÖ Script de instru√ß√µes conclu√≠do!\")\n",
    "print(\"   Siga os passos acima para salvar todos os arquivos\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
